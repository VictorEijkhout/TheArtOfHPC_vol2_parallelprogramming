% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2022
%%%%
%%%% mpi-started.tex : introductory chapter about MPI
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter you will learn the use of the main tool for 
distributed memory programming: the \acf{MPI} library.
The \ac{MPI} library has about 250 routines, many of which you may
never need. Since this is a textbook, not a reference manual, we will
focus on the important concepts and give the important routines for
each concept. What you learn here should be enough for most common
purposes. You are advised to keep a reference document handy, in
case there is a specialized routine, or to look up subtleties about
the routines you use.

\Level 0 {Distributed memory and message passing}

In its simplest form, a distributed memory machine is a collection of
single computers hooked up with network cables. In fact, this has a name:
a \indexterm{Beowulf cluster}. As you recognize from that setup, 
each processor can run an independent program, and has its own memory
without direct access to other processors' memory. MPI is the magic
that makes multiple instantiations of the same executable run
so that they know about each other and can exchange data through the 
network.

One of the reasons that MPI is so successful as a tool for high
performance on clusters is that it is very explicit: the programmer
controls many details of the data motion between the processors.
Consequently, a capable programmer can write very efficient code with MPI.
Unfortunately, that programmer will have to spell things out
in considerable detail. For this reason, people sometimes call MPI
`the assembly language of parallel programming'. If that sounds scary,
be assured that things are not that bad. You can get started 
fairly quickly with MPI, using just the basics,
and coming to the more sophisticated tools 
only when necessary.

Another reason that MPI was a big hit with programmers is that
it does not ask you to learn a new language: it is a library that 
can be interfaced to C/C++ or Fortran; there are even bindings to Python
and \indexterm{Java} (not described in this course).
This does not mean, however, that it is simple to
`add parallelism' to an existing sequential program.
An MPI version of a serial program takes considerable rewriting;
certainly more than shared memory parallelism through OpenMP,
discussed later in this book.

MPI is also easy to install: there are free implementations
that you can download and install on any computer that has a Unix-like
operating system, even if that is not a parallel machine.
However, if you are working on a supercomputer cluster,
likely there will already be an MPI installation,
tuned to that machine's network.

\Level 0 {History}

Before the MPI standard was developed in 1993-4, there were many
libraries for distributed memory computing, often proprietary
to a vendor platform. MPI standardized the inter-process communication
mechanisms. Other features, such as process management in \indexterm{PVM},
or parallel I/O were omitted. Later versions of the standard
have included many of these features.

Since MPI was designed by a large number of academic and commercial
participants, it quickly became a standard. A~few packages
from the pre-MPI era, such as \indexterm{Charmpp}~\cite{charmpp},
are still in use since they support mechanisms that do not exist
in MPI.

\Level 0 {Basic model}
\label{sec:mpiexec}

Here we sketch the two most common scenarios for using MPI. In the
first, the user is working on an interactive machine, which has
network access to a number of hosts, typically a network of workstations;
see figure~\ref{fig:mpi-interactive}.
\begin{figure}[ht]
  \includegraphics[scale=.12]{mpi-interactive}
  \caption{Interactive MPI setup}
  \label{fig:mpi-interactive}
\end{figure}
The user types the command \indextermtt{mpiexec}\footnote
{A command variant is \indexterm{mpirun}; your local cluster
  may have a different mechanism.}
and supplies
\begin{itemize}
\item The number of hosts involved,
\item their names, possibly in a hostfile,
\item and other parameters, such as whether to include the interactive
  host; followed by
\item the name of the program and its parameters.
\end{itemize}
The \indextermtt{mpiexec} program then makes an \n{ssh}\index{ssh} connection
to each of the hosts, giving them sufficient information that they 
can find each other. All the output of the processors is piped through the 
\indextermtt{mpiexec} program, and appears on the interactive console.

In the second scenario (figure~\ref{fig:mpi-batch}) the user prepares
a \indextermbus{batch}{job} script with commands, and these will be
run when the \indextermbus{batch}{scheduler} gives a number of hosts
to the job.
\begin{figure}[ht]
  \includegraphics[scale=.1]{mpi-batch}
  \caption{Batch MPI setup}
  \label{fig:mpi-batch}
\end{figure}
Now the batch script contains the \indextermtt{mpiexec} command%
\begin{istc}
, or some variant such as \indextermtt{ibrun}%
\end{istc}
, and the hostfile is dynamically generated when the job starts.
Since the job now runs at a time when the user may not be logged in, 
any screen output goes into an output file.

You see that in both scenarios the parallel program is started
by the \indextermtt{mpiexec} command using
an \ac{SPMD} mode of execution: all hosts execute the same program.
It is possible for different hosts to execute different programs,
but we will not consider that in this book.

There can be options and environment variables that are specific to
some MPI installations, or to the network.
\begin{itemize}
\item \indexterm{mpich} and its derivatives such as
  \indextermbus{Intel}{MPI} or \indextermbus{Cray}{MPI} have
  \indextermbus{mpiexec}{options}:
  \url{https://www.mpich.org/static/docs/v3.1/www1/mpiexec.html}
  %\url{https://wiki.mpich.org/mpich/index.php/Proposed_MPIEXEC_Extensions}
\end{itemize}

\Level 0 {Making and running an MPI program}

MPI is a library, called from programs in ordinary programming languages
such as C/C++ or Fortran. To compile such a program you use your regular
compiler:
\begin{verbatim}
gcc -c my_mpi_prog.c -I/path/to/mpi.h
gcc -o my_mpi_prog my_mpi_prog.o -L/path/to/mpi -lmpich
\end{verbatim}
However, MPI libraries may have different names between different
architectures, making it hard to have a portable makefile. Therefore,
MPI typically has shell scripts around your compiler call,
called \indextermtt{mpicc}, \indextermtt{mpicxx}, \indextermtt{mpif90}
for C/C++/Fortran respectively.
\begin{verbatim}
mpicc -c my_mpi_prog.c
mpicc -o my_mpi_prog my_mpi_prog.o
\end{verbatim}

If you want to know what \indextermttdef{mpicc} does,
there is usually an option that prints out its definition.
On a Mac with the \indexterm{clang} compiler:
\begin{verbatim}
$$ mpicc -show
clang -fPIC -fstack-protector -fno-stack-check -Qunused-arguments -g3 -O0 -Wno-implicit-function-declaration -Wl,-flat_namespace -Wl,-commons,use_dylibs -I/Users/eijkhout/Installation/petsc/petsc-3.16.1/macx-clang-debug/include -L/Users/eijkhout/Installation/petsc/petsc-3.16.1/macx-clang-debug/lib -lmpi -lpmpi
\end{verbatim}

\begin{remark}
  In \indexterm{OpenMPI}, these commands are 
  binary executables by default,
  but you can make it a shell script by passing the
  \n{--enable-script-wrapper-compilers} option at configure time.
\end{remark}

MPI programs can be run on many different architectures. Obviously it
is your ambition (or at least your dream) to run your code on a
cluster with a hundred thousand processors and a fast network. But
maybe you only have a small cluster with
plain \indexterm{ethernet}. Or maybe you're sitting in a plane, with
just your laptop. An MPI program can be run in all these
circumstances~--~within the limits of your available memory of course.

The way this works is that you do not start your executable directly,
but you use a program, typically called \indextermtt{mpiexec} or
something similar, which makes a connection to all available
processors and starts a run of your executable there. So if you have a
thousand nodes in your cluster, \indextermtt{mpiexec} can start your program once
on each, and if you only have your laptop it can start a few instances
there. In the latter case you will of course not get great
performance, but at least you can test your code for correctness.

\begin{tacc}
\begin{pythonnote}{Running mpi4py programs}
    Load the TACC-provided python:
\begin{verbatim}
module load python
\end{verbatim}
and run it as:
\begin{verbatim}
ibrun python-mpi yourprogram.py
\end{verbatim}
\end{pythonnote}
\end{tacc}

\Level 0 {Language bindings}

\Level 1 {C}
\index{C!MPI bindings|see{MPI, C bindings}}
\index{MPI!C bindings|(}

The MPI library is written in~C. 
However, the standard is careful to distinguish between MPI routines,
versus their \emph{C bindings}.
In fact, as of MPI~\mpistandard{4},
for a number of routines there are two bindings,
depending on whether you want 4~byte integers,
or larger.
See section~\ref{sec:mpi-bigdata},
in particular~\ref{sec:c-mpi-count}.

\index{MPI!C bindings|)}

\Level 1 {C++, including MPL}
\index{C++!bindings|see{MPI, C++ bindings}}
\index{MPI!C++ bindings|(}
\index{MPL|(textbf}

\emph{C++ bindings} were defined in the standard at one point,
but they were declared deprecated,
and have been officially removed in the 
\mpistandardsub{3}{C++ bindings removed} standard.
Thus, MPI can be used from C++ by including
\begin{verbatim}
#include <mpi.h>
\end{verbatim}
and using the C API.

The \indexterm{boost} library has its own version of MPI, but it seems
not to be under further development.  A~recent
effort at idiomatic C++ support is \indexac{MPL}
\url{https://github.com/rabauke/mpl}.
This book has an index of \ac{MPL} notes and commands:
section~\ref{sec:idx:mpl}.

\begin{mplnote}{Notes format}
MPL\index{MPL!compiling and linking}
is a C++ header-only library.
Notes on MPI usage from MPL will be indicated like this.
\end{mplnote}

\index{MPL|)}
\index{MPI!C++ bindings|)}

\Level 1 {Fortran}
\index{Fortran!MPI bindings|see{MPI, Fortran bindings}}
\index{Fortran!Fortran2008!MPI bindings|see{MPI, Fortran2008 bindings}}
\index{MPI!Fortran bindings|(}

\begin{fortrannote}{Formatting of Fortran notes}
  Fortran-specific notes will be indicated with a note like this.
\end{fortrannote}

Traditionally, \emph{Fortran bindings} for MPI look very much like the C~ones,
except that each routine has a final \indexterm{error return}
parameter.
You will find that a lot of MPI code in Fortran conforms to this.

\index{MPI!Fortran2008 bindings|(}
%
However, in the \emph{MPI~3}%
\index{MPI!MPI-3!Fortran2008 interface} standard it is recommended that
an MPI implementation providing a Fortran interface provide a
module named \indextermttdef{mpi_f08} that can be used in a Fortran program.
This incorporates the following improvements:
\begin{itemize}
\item
  This defines MPI routines to have an optional final parameter for the error.
\item 
  There are some visible implications of using the \n{mpi_f08} module,
  mostly related to the fact that some of the `MPI datatypes' such as
  \indexmpishow{MPI_Comm}, which were declared as \n{Integer}
  previously, are now a Fortran \n{Type}.
  See the following sections for details:
  Communicator~\ref{sec:comm-basic}, Datatype~\ref{sec:mpi-datatype},
  Info~\ref{sec:mpi-info}, Op~\ref{sec:mpi-op-create},
  Request~\ref{sec:nonblocking}, Status~\ref{sec:mpi-status},
  Window~\ref{sec:windows}.
\item
  The \indextermtt{mpi_f08} module solves a problem with previous
  \emph{Fortran90 bindings}\index{Fortran!Fortran90!bindings}:
  %
  Fortran90 is a strongly typed language, so it is not possible to pass
  argument by reference to their address, as C/C++ do with the \n{void*}
  type for send and receive buffers. This was solved by having
  separate routines for each datatype, and providing an \n{Interface} block
  in the MPI module. If you manage to request a version that does not exist,
  the compiler will display a message like
\begin{verbatim}
There is no matching specific subroutine for this generic subroutine call [MPI_Send]
\end{verbatim}
%
For details see
\url{http://mpi-forum.org/docs/mpi-3.1/mpi31-report/node409.htm}.
\end{itemize}

\index{MPI!Fortran2008 bindings|)}

\index{MPI!Fortran bindings|)}

\Level 1 {Python}
\label{sec:python-bind}
\index{Python!MPI bindigs|see{MPI, Python bindings}}
\index{MPI!Python bindings|(}

\begin{pythonnote}{Python notes}
  Python-specific notes will be indicated with a note like this.
\end{pythonnote}

The \indextermttdef{mpi4py} package~\cite{mpi4py:homepage}
of \emph{python bindings}
is not defined by the MPI standards committee.
Instead, it is the work of an individual,
\indextermsub{Lisandro}{Dalcin}.

In a way, the Python interface is the most elegant. It uses \ac{OO}
techniques such as methods on objects, and many default arguments.

Notable about the Python bindings is that many communication routines
exist in two variants:
\begin{itemize}
\item a version that can send arbitrary Python objects. These routines
  have lowercase names such as \n{bcast}; and
\item a version that sends \indexterm{numpy} objects; these routines
  have names such as \n{Bcast}. Their syntax can be slightly different.
\end{itemize}
The first version looks more `pythonic', is easier to write,
and can do things like sending python objects,
but it is also decidedly less efficient since data is packed
and unpacked with \n{pickle}. As a common sense guideline,
use the \n{numpy} interface in the performance-critical parts
of your code, and the pythonic interface only for complicated
actions in a setup phase.

Codes with \texttt{mpi4py} can be interfaced to other languages
through Swig or conversion routines.

Data in \texttt{numpy} can be specified as a simple object,
or \texttt{[data, (count,displ), datatype]}.

\index{MPI!Python bindings|)}

\Level 1 {How to read routine signatures}
\label{sec:protos}

Throughout the MPI part of this book we will give the reference syntax
of the routines. This typically comprises:
\begin{itemize}
\item The semantics: routine name and list of parameters and what they
  mean.
\item C synxtax: the routine definition as it appears in the
  \indextermtt{mpi.h} file.
\item Fortran syntax: routine definition with parameters, giving
  in/out specification.
\item Python syntax: routine name, indicating to what class it
  applies, and parameter, indicating which ones are optional.
\end{itemize}

These `routine signatures' look like code but they are not! Here is
how you translate them.

\Level 2 {C}

The typically C routine specification in MPI looks like:
\begin{lstlisting}
int MPI_Comm_size(MPI_Comm comm,int *nprocs)
\end{lstlisting}
This means that
\begin{itemize}
\item The routine returns an \n{int} parameter. Strictly speaking you
  should test against \indexmpishow{MPI_SUCCESS} (for all error codes,
  see section~\ref{sec:mpi-err-codes}):
\begin{lstlisting}
MPI_Comm comm = MPI_COMM_WORLD;
int nprocs;
int errorcode;
errorcode = MPI_Comm_size( MPI_COMM_WORLD,&nprocs);
if (errorcode!=MPI_SUCCESS) {
  printf("Routine MPI_Comm_size failed! code=%d\n",
         errorcode);
  return 1;
}
\end{lstlisting}
  However, the error codes are hardly ever useful, and there is not
  much your program can do to recover from an error. Most people call
  the routine as
\begin{lstlisting}
MPI_Comm_size( /* parameter ... */ );
\end{lstlisting}
For more on error handling, see section~\ref{sec:mpi:error}.
\item The first argument is of type \indexmpishow{MPI_Comm}. This is not a C
  built-in datatype, but it behaves like one. There are many of these
  \n{MPI_something} datatypes in MPI. So you can write:
\begin{lstlisting}
MPI_Comm my_comm =
    MPI_COMM_WORLD; // using a predefined value
MPI_Comm_size( comm, /* remaining parameters */ );
\end{lstlisting}
\item Finally, there is a `star' parameter. This means that the
  routine wants an address, rather than a value. You would typically write:
\begin{lstlisting}
MPI_Comm my_comm = MPI_COMM_WORLD; // using a predefined value
int nprocs;
MPI_Comm_size( comm, &nprocs );
\end{lstlisting}
  Seeing a `star' parameter usually means either: the routine has an
  array argument, or: the routine internally sets the value of a
  variable. The latter is the case here.
\end{itemize}

\Level 2 {Fortran}

The Fortran specification looks like:
\lstset{language=Fortran} %pyskip
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Type(MPI_Comm), Intent(In) :: comm
Integer, Intent(Out) :: size
Integer, Optional, Intent(Out) :: ierror
\end{lstlisting}
or for the \fstandard{90} legacy mode:
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
INTEGER, INTENT(IN) :: comm
INTEGER, INTENT(OUT) :: size
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
\end{lstlisting}
The syntax of using this routine is close to this specification: you
write
\begin{lstlisting}
Type(MPI_Comm) :: comm = MPI_COMM_WORLD
! legacy: Integer :: comm = MPI_COMM_WORLD
Integer :: comm = MPI_COMM_WORLD
Integer :: size,ierr
CALL MPI_Comm_size( comm, size ) ! without the optional ierr
\end{lstlisting}
\lstset{language=C} %pyskip

\begin{itemize}
\item Most Fortran routines have the same parameters as the
  corresponding C routine, except that they all have the error code as
  final parameter, instead of as a function result. As with~C, you can
  ignore the value of that parameter. Just don't forget it.
\item The types of the parameters are given in the specification.
\item Where C routines have \indexmpishow{MPI_Comm} and \indexmpishow{MPI_Request} and such
  parameters, Fortran has \n{INTEGER} parameters, or sometimes arrays
  of integers.
\end{itemize}

\Level 2 {Python}

The Python interface to MPI uses classes and objects. Thus, a
specification like:
\lstset{language=Python} %pyskip
\begin{lstlisting}
MPI.Comm.Send(self, buf, int dest, int tag=0)
\end{lstlisting}
should be parsed as follows.
\begin{itemize}
\item First of all, you need the \n{MPI} class:
\begin{lstlisting}
from mpi4py import MPI
\end{lstlisting}
\item Next, you need a \n{Comm} object. Often you will use the
  predefined communicator
\begin{lstlisting}
comm = MPI.COMM_WORLD
\end{lstlisting}
\item The keyword \n{self} indicates that the actual routine \n{Send}
  is a method of the \n{Comm} object, so you call:
\begin{lstlisting}
comm.Send( .... )
\end{lstlisting}
\item Parameters that are listed by themselves, such as \n{buf}, as
  positional. Parameters that are listed with a type, such as \n{int
    dest} are keyword parameters. Keyword parameters that have a value
  specified, such as \n{int tag=0} are optional, with the default
  value indicated. Thus, the typical call for this routine is:
\begin{lstlisting}
comm.Send(sendbuf,dest=other)
\end{lstlisting}
  specifying the send buffer as positional parameter, the destination
  as keyword parameter, and using the default value for the optional tag.
\end{itemize}
Some python routines are `class methods', and their specification
lacks the \n{self} keyword. For instance:
\begin{lstlisting}
MPI.Request.Waitall(type cls, requests, statuses=None)
\end{lstlisting}
would be used as
\begin{lstlisting}
MPI.Request.Waitall(requests)
\end{lstlisting}
\lstset{language=C} %pyskip

\Level 0 {Review}

\begin{review}
  What determines the parallelism of an MPI job?
  \begin{enumerate}
  \item The size of the cluster you run on.
  \item The number of cores per cluster node.
  \item\label{it:ans:mpiexec} The parameters of the MPI starter (\indextermtt{mpiexec},
    \n{ibrun},\ldots)
  \end{enumerate}
\end{review}

\begin{review}
  T/F: the number of cores of your laptop is the limit of how many MPI
  proceses you can start up.
\end{review}

\begin{review}
  Do the following languages have an object-oriented interface to MPI? In what sense?
  \begin{enumerate}
  \item C
  \item C++
  \item Fortran2008
  \item Python
  \end{enumerate}
\end{review}
