% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2023
%%%%
%%%% omp-parallel.tex : about parallel regions, nesting and stuff.
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{parallel region|(}

\Level 0 {Creating parallelism with parallel regions}

In OpenMP you need to indicate explicitly what passages are
parallel. Creating parallelism, which here means: creating a team of threads,
is done with the \indexpragma{parallel} pragma.
A~block preceded by the \n{omp parallel} pragma
is called a \emph{parallel region}; it
is executed by a newly created team of threads. 
This is an instance of the \indexac{SPMD} model: all threads execute the same
segment of code.
\begin{lstlisting}
#pragma omp parallel
{
  // this is executed by a team of threads
}
\end{lstlisting}

It would be pointless to have the block be executed identically by
all threads.
One way to get a meaningful parallel code is to use the function
\indexompshow{omp_get_thread_num} to find out which thread you are,
and execute work that is individual to that thread.
This function gives a number relative to the current team;
recall from figure~\ref{fig:forkjoin} that new teams can be created recursively.

There is also a function
\indexompshow{omp_get_num_threads} to find out the total number of threads.

The first thing we want to do is create a team of threads. This
is done with a \indexterm{parallel region}.
Here is a very simple example where each thread
outputs its number:
\csnippetwithoutput{hello-omp}{examples/omp/c}{hello}
or in Fortran
\fsnippetwithoutput{hello-omp-f}{examples/omp/f}{helloompf}

\begin{cppnote}{Output streams in parallel}
  \label{sl:par-stringstream}
  The use of \lstinline{cout} may give jumbled output:\slidebreak
  lines can break at each \verb+<<+.\slidebreak
  Use \indextermtt{stringstream} to form a single stream to output.
  
  \cxxverbatimsnippet[examples/omp/cxx/hello.cxx]{hello-omp-cxx}
\end{cppnote}

\begin{cppnote}{Parallel regions in lambdas}
  OpenMP parallel regions can be in functions,
  including lambda expressions.
  \cxxverbatimsnippet{ompinlambda}
  (`Immediately Invoked Function Expression')
\end{cppnote}

The following example uses parallelism for an actual calculation:
\begin{lstlisting}
result = f(x)+g(x)+h(x)
\end{lstlisting}
you could parallelize this as
\begin{lstlisting}
double result,fresult,gresult,hresult;
#pragma omp parallel
{ int num = omp_get_thread_num();
  if (num==0)      fresult = f(x);
  else if (num==1) gresult = g(x);
  else if (num==2) hresult = h(x);
}
result = fresult + gresult + hresult;
\end{lstlisting}

This code corresponds to the model we just discussed:
\begin{itemize}
\item Immediately preceding the parallel block, one thread will be
  executing the code.
  In the main program this is the \indextermsubdef{initial}{thread}.
\item At the start of the block, a new \indextermsub{team of}{threads} is
  created, and the thread that was active before the block
  becomes the \indextermsubdef{master}{thread} of that team.
\item After the block only the master thread is active.
\item Inside the block there is team of threads: each thread in the
  team executes the body of the block, and it will have access to all
  variables of the surrounding environment.
  How many
  threads there are can be determined in a number of ways; we will get to that later.
\end{itemize}

\begin{remark}
  In future versions of OpenMP, the master thread will be called
  the \indextermsubdef{primary}{thread}.
  In 5.1 the master construct will be deprecated, and \indexompshow{masked} (with
  added functionality) will take its place.  In 6.0 master will
  disappear from the Spec, including \indextermtt{proc_bind} master “variable”
  and combined master constructs (master taskloop, etc.)
\end{remark}

\begin{exercise}
  What happens if you call \indexompshow{omp_get_thread_num} and \indexompshow{omp_get_num_threads}
  outside a parallel region?
\end{exercise}

\Level 0 {Nested parallelism}
\label{sec:omp-levels}
\index{nested parallelism|(}

What happens if you call a function from inside a parallel region, and
that function itself contains a parallel region?
\begin{lstlisting}
int main() {
  ...
#pragma omp parallel
  {
  ...
  func(...)
  ...
  }
} // end of main
void func(...) {
#pragma omp parallel
  {
  ...
  }
}
\end{lstlisting}

Since any thread can create a team, you may expect that every thread,
in its call to \lstinline{func},
will create its own new team.
This is called \indextermsub{nested}{parallelism}
and it works as described.

However, by default, the nested parallel region will have only one thread.
You need to allow non-trivial nested parallelism explicitly.

To allow nested thread creation,
use the environment variable
\indexompdef{OMP_MAX_ACTIVE_LEVELS} (default:~1)
to set the number of levels of parallel nesting.
Equivalently, there are functions
\indexompdef{omp_set_max_active_levels} and \indexompdef{omp_get_max_active_levels}:
\begin{verbatim}
OMP_MAX_ACTIVE_LEVELS=3
\end{verbatim}
or
\begin{lstlisting}
void omp_set_max_active_levels(int);
int omp_get_max_active_levels(void);
\end{lstlisting}

\begin{remark}
  A deprecated mechanism is to set the environment variable
  \indexompdepr{OMP_NESTED} (default: false)
  or its corresponding function:
\begin{verbatim}
OMP_NESTED=true
 or
omp_set_nested(1)
\end{verbatim}
\end{remark}

Nested parallelism can happen with nested loops,
but it's also possible to have a \indexompshow{sections} construct
and a loop nested.
Example:
\csnippetwithoutput{ompsectionnest}{code/omp/c}{sectionnest}

The amount of nested parallelism can be set:
\begin{verbatim}
OMP_NUM_THREADS=4,2
\end{verbatim}
means that initially a parallel region will have four threads, and
each thread can create two more threads.
It is still necessary that set the number of active levels.

The total number of threads active simultaneously
(technically: in a \indexterm{contention group})
can be limited with:
\begin{verbatim}
  OMP_THREAD_LIMIT=123
\end{verbatim}
Its value can be queried with \indexompshow{omp_get_thread_limit}.

More functions: \indexompshow{omp_get_level}, \indexompshow{omp_get_active_level},
\indexompshow{omp_get_ancestor_thread_num}, \indexompshow{omp_get_team_size}\lstinline{(level)}.

\Level 1 {Subprograms with parallel regions}

A common application of nested parallelism is the case
where you have a subprogram with a parallel region,
which itself gets called from a parallel region.

\begin{exercise}
  Test nested parallelism by writing an OpenMP program as follows:
  \begin{enumerate}
  \item Write a subprogram that contains a parallel region.
  \item\label{ex:nest:sub} Write a main program with a parallel region; call the subprogram both inside and outside the parallel region.
    \item Insert print statements 
      \begin{enumerate}
      \item in the main program outside the parallel region,
      \item in the parallel region in the main program,
      \item\label{ex:nest:sub:sub} in the subprogram outside the parallel region,
      \item in the parallel region inside the subprogram.
      \end{enumerate}
  \end{enumerate}
  Run your program and count how many print statements of each type you get.
\end{exercise}

Writing subprograms that are called in a parallel region illustrates
the following point: directives are evaluation with respect to the
\emph{dynamic scope}\index{parallel region!dynamic scope} of the
parallel region, not just the lexical scope. In the following example:
\begin{lstlisting}
#pragma omp parallel
{
  f();
}
void f() {
#pragma omp for
  for ( .... ) {
    ...
  }
}
\end{lstlisting}
the body of the function~\n{f} falls in the dynamic scope of the
parallel region, so the for loop will be parallelized.

If the function may be called both from inside and outside parallel
regions, you can test which is the case with \indexompshow{omp_in_parallel}.

\begin{cppnote}{Dynamic scope for class methods}
  Dynamic scope holds for class methods as for any other function:
  %
  \def\snippetcodefraction{.4}
  \def\snippetlistfraction{.55}
  \cxxsnippetwithoutput{nestclass}{examples/omp/cxx}{nested}
\end{cppnote}

\index{nested parallelism|)}
\index{parallel region|)}

\Level 0 {Cancel parallel construct}
\label{sec:omp-cancel}

It is possible to terminate a parallel construct early
with the \indexompdef{cancel} directive:
\begin{lstlisting}
!$omp cancel construct [if (expr)]
\end{lstlisting}
where construct is
\lstinline{parallel},
\lstinline{sections},
\lstinline{do}
or
\lstinline{taskgroup}.

See section~\ref{sec:omp-dfs-example} for an example.

Cancelling is disabled by default for performance reasons.
To activate it, set the \indexompdef{OMP_CANCELLATION} variable to true.

The state of cancellation can be queried with \indexompdef{omp_get_cancellation},
but there is no function to set it.

Cancellation can happen at most obvious places where OpenMP is active,
but additional cancellation points can be set with 
\begin{lstlisting}
#pragma omp cancellation point <construct>
\end{lstlisting}
where the construct is \indexompshow{parallel}, \indexompshow{sections},
\indexompshow{for}, \indexompshow{do}, \indexompshow{taskgroup}.

\Level 0 {Review questions}

\begin{exercise}
  T/F?
  The function \lstinline+omp_get_num_threads()+
  returns a number that is equal to the number of cores.
\end{exercise}

\begin{exercise}
  T/F?
  The function \lstinline+omp_set_num_threads()+
  can not be set to a higher number than the number of cores.
\end{exercise}

\begin{exercise}
  What function can be used to detect the number of cores? 
\end{exercise}
