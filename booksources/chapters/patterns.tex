%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012/3
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The basic problem in designing a parallel algorithm is to find the proper assignment
of work to processes. We discuss some possibilities.

\Level 0 {Fully parallel work}

Some algorithms involve operations that are truly independent.
For instance, if you add two vectors
\[ \bar z\leftarrow \bar x + \bar y, \]
all the element additions
\[ z_i\leftarrow x_i+y_i \]
are independent. Thus, if we had as many processors as vector elements,
each could do one addition.
We could write this as:
\begin{verbatim}
parallel_for p in Processors:
    i = index_of_processor()
    z[i] = x[i]+y[i]
\end{verbatim}

\Level 0 {Introduction: Quicksort}

Here is a simple recursive implementation of quicksort.
\begin{verbatim}
def QuickSort( array )
    if len(array)==1:
        return array
    else:
        median = find_median( array )
        left_array = [ x for x in array if x<median ]
        right_array = [ x for x in array if x>=median ]
        return QuickSort( left_array ) + QuickSort( right_array )
def find_median( array ):
    return array[0] # simple solution, probably bad.
\end{verbatim}
The important steps are:
\begin{itemize}
\item Finding the median. We can do this in a simple way, which is potentially bad, or
  in more sophisticated ways; however, no general statement about the complexity of this can be made,
  so we'll just ignore this;
\item Splitting the array in two halves;
\item Sorting the two subarrays.
\end{itemize}
There is clear parallelism in this algorithm: the two recursive calls
are independent and can therefore be done in parallel.

Let's attempt a complexity analysis of this parallelization scheme.
We start with an array of length~$N$, and we assume that we always
find the perfect median at zero cost.  In that case, the first sort
call takes time~$N$ to split the array in two halves; the simultaneous
recursive calls take time $N/2$ each to split their subarrays, et
cetera.  We see that the running time is approximately~$2N$, which is
better than the originally expected $N\log N$, but not a whole lot.

The problem is of course that splitting the array is not done in parallel,
although it probably could be. Here is how we could do it with two processors:
\begin{verbatim}
N = len(array)
new_array = [ None for i in range(N) ]
low_pointer = 0
high_pointer = N
median = find_median( array )

def make_left_array():
    global array,new_array
    for i in range(N):
        if array[i]<median:
            new_array[low_pointer] = array[i]
            low_pointer += 1
    return new_array[:low_pointer]

def make_right_array():
    global array,new_array
    for i in range(N):
        if array[i]>=median:
            high_pointer -= 1
            new_array[high_pointer] = array[i]
    return new_array[high_pointer:]
\end{verbatim}
In this code, one process finds the elements below the median, and the other one
find elements at least as large as the median.
Now we are indeed using two processes, but they each still go through the whole array,
so the running time is still~$N$.

(There is a further problem with this algorithm, namely that it uses
twice an extra $N$ memory locations for each level of recursive
function calls. This can be eliminated by the `Dutch National Flag'
algorithm~\url{http://en.wikipedia.org/wiki/Dutch_national_flag_problem},
but this needlessly complicates the discussion.)

As a way of halfing the amount of work per process, consider
a solution where both processes find elements of both type, but they 
each consider only half the array.
\begin{verbatim}
N = len(array)
new_array = [ None for i in range(N) ]
low_pointer = 0
high_pointer = N-1
median = find_median( array )

def make_left_array():
    global array,new_array
    for i in range(0:N:2):
        place_element[i]
    return new_array[:low_pointer]

def make_right_array():
    global array,new_array
    for i in range(1:N:2):
        place_element[i]
    return new_array[high_pointer:]

def place_element( i ):
    if array[i]<median:
        new_array[low_pointer] = array[i]
        low_pointer += 1
    elif array[i]>=median:
        high_pointer -= 1
        new_array[high_pointer] = array[i]
\end{verbatim}
Now both processes access shared data which has many problems associated with it.

\Level 0 {Data partitioning}

In the previous section we looked at the parallelization of quicksort
purely by considering a division of the work. We will now look at strategies
of parallelizing by looking at the data involved.

Certain problems can be described as constructing an output data object
consisting of many elements, with often independence of the element calculations.
Linear algebra is a source of such examples:
\begin{itemize}
\item In the matrix-vector product every element of the output vector
  can be computed individually.
\item The same holds for the matrix-matrix product.
\item On the other hand, computing and applying an $LU$ factorization
  (see \HPSCref{sec:lu-fact}) has a great deal of sequentiality. But even there,
  sub-operations such as updating a matrix block with a low rank matrix
  show a great deal of parallelism.
\end{itemize}

From this description it is clear that one obvious partitioning scheme
would be to partition the output over the processes. This is known
as the \indexterm{owner computes} scheme: each output element is uniquely
owned by one process, which performs all or most of the computations
for constructing that element.

