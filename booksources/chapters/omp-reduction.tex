% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2024
%%%%
%%%% omp-reduction.tex : reduction loops
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{omp!reduction|(}
\label{sec:reduction}

\Level 0 {Reductions: why, what, how?}
\label{sec:reduct-intro}

Parallel tasks often produce some quantity that needs to be summed
or otherwise combined.
If you write:
\begin{lstlisting}[language=omp]
  int sum=0;
  #pragma omp parallel for
  for (int i=0; i<N; i++)
    sum += f(i);
\end{lstlisting}
you will find that the \lstinline{sum} value depends on the number of threads,
and is likely not the same as when you execute the code sequentially.
The problem here is the \indexterm{race condition} involving the \n{sum}
variable, since this variable is shared between all threads.

We will discuss several strategies of dealing with this.

\Level 1 {Reduction clause}

The easiest way to effect a reduction is of course to use the \indexclause{reduction}
clause.
Adding this to an \n{omp parallel} region
has the following effect:
\begin{itemize}
\item OpenMP will make a copy of the reduction variable per thread,
  initialized to the identity of the reduction operator, for instance
  $1$~for multiplication.
\item Each thread will then reduce into its local variable;
\item At the end of the parallel region, the local results are combined, again
  using the reduction operator, into the global variable.
\end{itemize}

The simplest case is a reduction over a parallel loop.
Here we compute $\pi/4$ as a \indextermbus{Riemann}{sum}:

\cverbatimsnippet[examples/omp/c/reductpar.c]{pi4reduct}

You can also reduce over \indexompshow{sections}:
%
\cverbatimsnippet{ompsectreduct}

Another reduction, this time over a parallel region,
without any work sharing:
%
\cverbatimsnippet[examples/omp/c/reductpar.c]{reductpar}

If you want to reduce multiple variables with the same operator, use
\begin{lstlisting}[language=omp]
reduction(+:x,y,z)
\end{lstlisting}
For multiple reduction with different operators, use more than one clause.

\begin{remark}
  A reduction is one of those cases where the parallel execution can have a slightly different
  value from the one that is computed sequentially, because floating point operations
  are not associative, so \emph{roundoff}\index{omp!reduction!roundoff}
  will lead to differing results.
  See~\HPSCref{sec:roundoff-parallel} for more explanation.

  The OpenMP standard does not even specify that two runs with the same number of threads,
  and the same scheduling, have to give identical results.
  Some runtimes may have a setting to enforce this;
  for instance \indextermtt{KMP_DETERMINISTIC_REDUCTION} for the Intel runtime.
\end{remark}

\Level 1 {Code your own reduction}

While using a \lstinline[language=omp]{reduction} clause is the preferred way of dealing
with codes as in section~\ref{sec:reduct-intro} above,
it can be instructive to look at other mechanisms.

The most immediate way is to eliminate the race condition
by declaring a \indexterm{critical section}:
\begin{lstlisting}[language=omp]
double result = 0;
#pragma omp parallel
{
  double local_result;
  int num = omp_get_thread_num();
  if (num==0)      local_result = f(x);
  else if (num==1) local_result = g(x);
  else if (num==2) local_result = h(x);
# pragma omp critical
  result += local_result;
}
\end{lstlisting}

This is a good solution if the amount of serialization in the critical section
is small compared to computing the functions~$f,g,h$. On the other hand, you
may not want to do that in a loop:
\begin{lstlisting}[language=omp]
double result = 0;
#pragma omp parallel
{
  double local_result;
# pragma omp for
  for (i=0; i<N; i++) {
    local_result = f(x,i);
#   pragma omp critical
    result += local_result;
  } // end of for loop
}
\end{lstlisting}
\begin{exercise}
  Can you think of a small modification of this code, that still uses a critical section,
  that is more efficient? Time both codes.
\end{exercise}

\Level 2 {False sharing}

If your code can not be easily structured as a reduction, you can 
realize the above scheme by hand by
`duplicating' the global variable and gather the contributions later.
This example presumes three threads, and gives each a location of their
own to store the result computed on that thread:
\begin{lstlisting}[language=omp]
double result,local_results[3];
#pragma omp parallel
{
  int num = omp_get_thread_num();
  if (num==0)      local_results[num] = f(x)
  else if (num==1) local_results[num] = g(x)
  else if (num==2) local_results[num] = h(x)
}
result = local_results[0]+local_results[1]+local_results[2]
\end{lstlisting}
While this code is correct, it may be inefficient because of a
phenomemon called \indexterm{false sharing}. Even though the threads write
to separate variables, those variables are likely to be on the same 
\indexterm{cacheline} (see \HPSCref{sec:falseshare} for an explanation).
This means that the cores will be wasting a lot of time and bandwidth updating
each other's copy of this cacheline.

False sharing can be prevent by giving each thread its own cacheline:
\begin{lstlisting}[language=omp]
double result,local_results[3][8];
#pragma omp parallel
{
  int num = omp_get_thread_num();
  if (num==0)      local_results[num][1] = f(x)
// et cetera
}
\end{lstlisting}
A more elegant solution gives each thread a true local variable,
and uses a critical section to sum these, at the very end:
\begin{lstlisting}[language=omp]
double result = 0;
#pragma omp parallel
{
  double local_result;
  local_result = .....
  #pragam omp critical
    result += local_result;
}
\end{lstlisting}

\Level 1 {Exercises}

\begin{exercise}
  \label{ex:omp-pi}
  Compute $\pi$ by \indexterm{numerical integration}. We use the fact that $\pi$
  is the area of the unit circle, and we approximate this by computing
  the area of a quarter circle using \indexterm{Riemann sums}.
  \begin{itemize}
  \item Let $f(x)=\sqrt{1-x^2}$ be the function that describes the
    quarter circle for $x=0\ldots 1$;
  \item Then we compute \[ \pi/4\approx\sum_{i=0}^{N-1} \Delta x
    f(x_i) \qquad \hbox{where $x_i=i\Delta x$ and $\Delta x=1/N$} \]
  \end{itemize}
  Write a program for this, and parallelize it using OpenMP parallel
  for directives.
  \begin{enumerate}
  \item Put a \indexompshow{parallel} directive around your loop. Does it still
    compute the right result? Does the time go down with the number of
    threads? (The answers should be no and no.)
  \item Change the \lstinline[language=omp]{parallel} to \lstinline[language=omp]{parallel for}
    (or \lstinline[language=omp]{parallel do}). Now is the result correct? Does execution speed up? (The
    answers should now be no and yes.)
  \item Put a \indexompshow{critical} directive in front of the update. (Yes and
    very much no.)
  \item Remove the \indexompshow{critical} and add a clause
    \indexompshow{reduction}\n{(+:quarterpi)} to the \indexompshow{for} directive.
    Now it should be correct and efficient.
  \end{enumerate}
  Use different numbers of cores and compute the
  speedup you attain over the sequential computation. Is there a
  performance difference between the OpenMP code with 1~thread and the
  sequential code?
\end{exercise}

\begin{remark}
  In this exercise you may have seen the runtime go up a couple of times
  where you weren't expecting it. The issue here is \indexterm{false
    sharing}; see~\HPSCref{sec:roundoff-parallel} for more explanation.
\end{remark}

\begin{exercise}[jacobi]
  \label{ex:omp-jacobi1}
  The Jacobi method for solving linear system $Ax=b$ is given by
  \[ x^{(n+1)}_i = a_{ii}\left(b_i-\sum_{j\not=i}a_{ij}x^{(n)}_j\right) \]
  Insert OpenMP directives in the code, and check that it converges
  with the same precision, regardless the number of threads.

  Study speedup. Does the problem size play a role?
\end{exercise}

\begin{exercise}
  \label{ex:omp-jacobi2}
  How much performance improvement do you get from considering
  \begin{itemize}
  \item removing barriers by \lstinline[language=omp]{nowait} clauses
  \item affinity
  \item first-touch
  \end{itemize}
\end{exercise}

\begin{exercise}
  \label{ex:omp-jacobi3}
  Experiment with scheduling options.\\
  Do you see any effect?\\
  In particular try small chunk sizes.
\end{exercise}

\begin{exercise}
  \label{ex:omp-jacobi4}
  Can you put the whole iteration loop in a parallel region?

  Does this give further performance improvement?
\end{exercise}

\Level 0 {Built-in reduction}

\Level 1 {Operators}

Arithmetic reductions: \lstinline{+,*,-,max,min}.
The minus operator is deprecated as of \ompstandard{5.2}.

Logical operator reductions in C: \n{& && | || ^}

Logical operator reductions in Fortran:
\n{.and. .or. .eqv. .neqv. .iand. .ior. .ieor.}

Reduction can be applied to any type for which the operator is defined.
The types to which \lstinline{max/min} are applicable are limited.

\begin{exercise}
  The maximum and minimum reductions were not added to OpenMP until
  \ompstandard{3.1}.
  Write a parallel loop that computes the maximum and
  minimum values in an array without using the \indexompshow{reduction} directive.
  Discuss the various options. Do timings
  to evaluate the speedup that is attained and to find the best option.
\end{exercise}

\Level 1 {Reduction on arrays}
\label{sec:omp-array-reduct}

Starting with the \ompstandard{4.5} standard, you can reduce
on statically and dynamically allocated arrays:
\cverbatimsnippet{creductstatic}
\cverbatimsnippet{creductdynamic}

\begin{cppnote}{Reductions on vectors}
  Use the \lstinline{data} method to extract the array
  on which to reduce. However, this does not work:
\begin{lstlisting}[language=omp]
vector<float> x;
#pragma omp parallel reduction(+:x.data())
\end{lstlisting}
  because the reduction clause wants a variable, not an expression,
  for the array, so you need an extra bare pointer:
  %
  \cxxverbatimsnippet{cxxreductvector}
\end{cppnote}

In the course of the reduction, OpenMP may need to allocate
temporary arrays. This may run into limitations of stack size.
You may need to increase the value of the \indexompshow{OMP_STACKSIZE}
environment variable.

\Level 0 {Initial value for reductions}

The treatment of initial values in reductions is slightly involved.
\begin{lstlisting}[language=omp]
  x = init_x
#pragma omp parallel for reduction(min:x)
  for (int i=0; i<N; i++)
    x = min(x,data[i]);
\end{lstlisting}
Each thread does a partial reduction, but its initial value is not the
user-supplied \n{init_x} value, but a value dependent on the
operator. In the end, the partial results will then be combined with
the user initial value.
The initialization values are mostly self-evident, such as zero for
addition and one for multiplication. For min and max they are
respectively the maximal and minimal representable value of the result type.

\begin{figure}[ht]
  \includegraphics[scale=.1]{omp-reduct}
  \caption{Reduction of four items on two threads, taking into account
    initial values.}
  \label{fig:omp-reduct}  
\end{figure}
%
Figure~\ref{fig:omp-reduct} illustrates this, where \n{1,2,3,4} are
four data items, \n{i}~is the OpenMP initialization, and \n{u}~is the
user initialization; each \n{p}~stands for a partial reduction value.
The figure is based on execution using two threads.

\begin{exercise}
  Write a program to test the fact that the partial results
  are initialized to the unit of the reduction operator.
\end{exercise}

\Level 0 {User-defined reductions}
\index{omp!reduction!user-defined|(textbf}

In a loop that performs a reduction,
most of the element-by-element reduction as done in user code.
However, in a parallel version of that loop,
OpenMP needs to perform that same reduction on the partial results
from the threads.
Thus, if you want to perform your own reduction,
you need to declare this reduction to OpenMP.

With \emph{user-defined reductions}, the programmer specifies the
function that does the elementwise comparison.
We discuss two strategies:
\begin{enumerate}
\item In non-\ac{OO} languages you can define a function,
  and declare that to be a reduction operator with the
  \indexomppragma{declare}~\indexclause{reduction} construct.
\item In \ac{OO} languages (C++ and \fstandard{2003})
  you can overload ordinary operators for types, including class objects.
\end{enumerate}

\Level 1 {Reduction functions}

This takes two steps.
\begin{enumerate}
\item You need a function of two arguments that returns the result of
  the comparison. You can do this yourself, but, especially with the
  C++ standard library, you can use functions such as \n{std::vector::insert}.
\item Specifying how this function operates on two variables
  \indexompshow{omp_out} and \indexompshow{omp_in}, corresponding to the
  partially reduced result and the new operand respectively. The new
  partial result should be left in \n{omp_out}.
\item Optionally, you can specify the value to which the reduction
  should be initialized.
\end{enumerate}

This is the syntax of the definition of the reduction, which can then
be used in multiple \indexclause{reduction} clauses.
\begin{lstlisting}[language=omp]
#pragma omp declare reduction 
    ( identifier : typelist : combiner )
    [initializer(initializer-expression)]
\end{lstlisting}
where:
\begin{description}
  \item[\texttt{identifier}] is a name; this can be overloaded for
    different types, and redefined in inner scopes.
  \item[\texttt{typelist}] is a list of types.
  \item[\texttt{combiner}] is an expression that updates the internal
    variable \indexompshow{omp_out} as function of itself and \indexompshow{omp_in}.
  \item[\texttt{initializer}] sets \indexompshow{omp_priv} to the
    identity of the reduction; this
    can be an expression or a brace initializer.
\end{description}

\begin{fortrannote}{Reduction declaration}
  The declaration statement has to be in the declaration section of your subprogram.
\end{fortrannote}

\Level 2 {Explicit expressions}

For very simple cases:
%
\cverbatimsnippet{ompminabsloop}
%
you can declare the reduction through an expression:
%
\cverbatimsnippet{ompminabsdef}
%
and use that in the \indexclause{reduction} clause:
%
\cverbatimsnippet{ompminabsclause}
%
for the above loop
%
\cverbatimsnippet{ompminabsloop}

\begin{cppnote}{Lambda expressions in declared reductions}
  You can use lambda expressions in the explicit expression
  for a declared reduction:
  %
  \cxxverbatimsnippet{cppreductlambda}
  %
  You can not assign the lambda expression to a variable and use that,
  because \lstinline[language=omp]{omp_in/omp_out} are the only variables allowed
  in the explicit expression.
\end{cppnote}

\Level 2 {Reduction functions}

For instance, recreating the maximum reduction would look like this:
%
\cverbatimsnippet[examples/omp/c/ireduct.c]{ompmymax}

\begin{exercise}
  Write a reduction routine that operates on an array of nonnegative
  integers, finding the smallest nonzero one. If the array has size
  zero, or entirely consists of zeros, return~\n{-1}.
\end{exercise}

\begin{cppnote}{Reduction over iterators}
  Support for
  \emph{C++ iterators}\index{C++ iterators!in OMP reduction}
\begin{lstlisting}[language=omp][language=C++]
#pragma omp declare reduction \
    (merge             // identifier 
    : std::vector<int> // typelist
    : omp_out.insert(omp_out.end(), omp_in.begin(), 
                     omp_in.end()) // combiner
    ) 
\end{lstlisting}
\end{cppnote}

\begin{cppnote}{Templated reductions}
  You can reduce with a templated function
  if you put both the declaration and the reduction
  in the same templated function:
  %
  \cxxverbatimsnippet{ompreducttemplate}
  %
  which is then called with specific data:
  %
  \cxxverbatimsnippet{ompreducttcall}
\end{cppnote}

\begin{cppnote}{Example: reduction over a map}
  Reduction over a \lstinline{std::map}
  by merging thread-local maps:
  \begin{multicols}{2}
    \cxxverbatimsnippet{ompbincounter}
    \columnbreak
    \cxxverbatimsnippet{ompbinreduce}
  \end{multicols}
\end{cppnote}

\Level 1 {Overloaded operators}

\begin{fortrannote}{Reductions on derived types}
  Reduction can be applied to any derived type that has the
  reduction operator defined.
  \begin{multicols}{2}
    \fverbatimsnippet{ftypeopdef}
    \columnbreak
    \fverbatimsnippet{ftypeopuse}
  \end{multicols}
  But note the extra \lstinline[language=omp]{declare} clause.  
\end{fortrannote}

\begin{cppnote}{Reduction on class objects}
  Reduction can be applied to any class for which the
  reduction operator is defined as \lstinline{operator+}
  or whichever operator the case may be.
  \begin{multicols}{2}
    \let\snippetoutputsize\footnotesize
    \cxxverbatimsnippet{ompclassop}
    \columnbreak
    \cxxverbatimsnippet{ompreductop}
  \end{multicols}
  A default constructor is required for the
  internally used init value;
  see figure~\ref{fig:omp-reduct}.
\end{cppnote}

\index{omp!reduction!user-defined|)}

\Level 0 {Scan / prefix operations}

A `scan' or \indextermbus{prefix}{operation}
is like a reduction, except that you're interested
in the partial results.
For this OpenMP, as of \ompstandard{5.0}, has the \indexompdef{scan} directive.
This needs the following:
\begin{itemize}
\item The reduction clause gets a modifier \indexompclause{inscan}:
\begin{lstlisting}[language=omp]
#pragma omp parallel for reduction(inscan,+:sumvar)
\end{lstlisting}
\item In the body of the parallel loop there is a \indexompshow{scan} directive
  that allows you to store the partial results.
  For inclusive scans the reduction variable is updated before the \indexompshow{scan} pragma:
\begin{lstlisting}[language=omp]
  sumvar // update
  #pragma omp scan inclusive(sumvar)
  partials[i] = sumvar
\end{lstlisting}
  For exclusive scans the reduction variable is updated after the \indexompshow{scan} pragma:
\begin{lstlisting}[language=omp]
  partials[i] = sumvar
  #pragma omp scan inclusive(sumvar)
  sumvar // update
\end{lstlisting}
\end{itemize}

\csnippetwithoutput{scanincexc}{examples/omp/c}{scansum}

\Level 0 {Reductions and floating-point math}

The mechanisms that OpenMP uses to make a reduction parallel go
against the strict rules for floating point expression evaluation in~C;
see~\HPSCref{sec:round-compile}. OpenMP ignores this issue: it is the
programmer's job to ensure proper rounding behavior.

\index{omp!reduction|)}

\Level 0 {Reductions in C++ standard algorithms}

\begin{cppnote}{Reductions on parallel standard algorithms}
  In section~\ref{sec:omp-vs-cpp-tbb} you saw how certain loop constructs
  can be realized in C++ through the \indexterm{execution policy} argument
  of the standard algorithms. The same holds for reductions.
\end{cppnote}

\cxxverbatimsnippet{reduceprimeomp}
\cxxverbatimsnippet{reduceprimecpp}

%% \lstinputlisting{examples/tbb/cxx/reducepolicy-scaling-ls6.runout}
\begingroup \tiny
\begin{verbatim}
Threads:  1
TBB:  Time:      391 msec
Stat: Time:      390 msec
Dyn:  Time:      389 msec

Threads: 25
TBB:  Time:       20 msec
Stat: Time:       17 msec
Dyn:  Time:       17 msec

Threads: 51
TBB:  Time:       13 msec
Stat: Time:        9 msec
Dyn:  Time:        8 msec

Threads: 76
TBB:  Time:       14 msec
Stat: Time:        8 msec
Dyn:  Time:        5 msec

Threads: 102
TBB:  Time:       76 msec
Stat: Time:        5 msec
Dyn:  Time:        4 msec

Threads: 128
TBB:  Time:       80 msec
Stat: Time:        4 msec
Dyn:  Time:        3 msec
\end{verbatim}
\endgroup

\endinput

\begin{verbatim}
================ #threads = 1 ================
               Sequential: 1.761771e+01; total force: 7.552465e+08
       Full loop Parallel: 3.251586e+00; total force: 7.552465e+08, speedup= 5.42
 Triangular update atomic: 1.787765e+01; total force: 7.552465e+08, speedup= 0.99
         Full loop atomic: 2.001587e+01; total force: 7.552465e+08, speedup= 0.88
================ #threads = 18 ================
               Sequential: 1.764474e+01; total force: 7.426427e+08
       Full loop Parallel: 1.820593e+00; total force: 7.426427e+08, speedup= 9.69
 Triangular update atomic: 2.513533e+00; total force: 7.426427e+08, speedup= 7.02
         Full loop atomic: 1.117391e+00; total force: 7.426427e+08, speedup=15.79
================ #threads = 37 ================
               Sequential: 1.764488e+01; total force: 7.383082e+08
       Full loop Parallel: 4.445617e+00; total force: 7.383082e+08, speedup= 3.97
 Triangular update atomic: 1.560774e+00; total force: 7.383082e+08, speedup=11.31
         Full loop atomic: 5.465267e-01; total force: 7.383082e+08, speedup=32.29
================ #threads = 56 ================
               Sequential: 1.764368e+01; total force: 7.420574e+08
       Full loop Parallel: 8.536602e+00; total force: 7.420574e+08, speedup= 2.07
 Triangular update atomic: 1.221944e+00; total force: 7.420574e+08, speedup=14.44
         Full loop atomic: 3.623484e-01; total force: 7.420574e+08, speedup=48.69
\end{verbatim}

