% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2022
%%%%
%%%% omp-affinity.tex : thread affinity
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{thread!affinity|(}

\Level 0 {OpenMP thread affinity control}
\label{sec:omp-proc-bind}

The matter of thread affinity becomes important on \emph{multi-socket nodes}%
\index{affinity!thread!on multi-socket nodes};
see the example in section~\ref{sec:first-touch}.

Thread placement can be controlled with two environment variables:
\begin{itemize}
\item the environment variable \indexompdef{OMP_PROC_BIND}
  describes how threads are bound to \indextermbus{OpenMP}{places}; while
\item the variable \indexompshow{OMP_PLACES} describes these places
  in terms of the available hardware.
\item When you're experimenting with these variables it is a good idea
  to set \indexompdef{OMP_DISPLAY_ENV} to true, so that OpenMP will
  print out at runtime how it has interpreted your specification.
  The examples in the following sections will display this output.
\end{itemize}

\Level 1 {Thread binding}
\label{omp:threadbind}

The variable \indexompdef{OMP_PLACES} defines a series of places to
which the threads are assigned.

Example: if you have two sockets and you define
\begin{verbatim}
OMP_PLACES=sockets
\end{verbatim}
then
\begin{itemize}
\item thread 0 goes to socket 0,
\item thread 1 goes to socket 1,
\item thread 2 goes to socket 0 again,
\item and so on.
\end{itemize}
On the other hand, if the two sockets have a total of sixteen cores
and you define
\begin{verbatim}
OMP_PLACES=cores
OMP_PROC_BIND=close
\end{verbatim}
then
\begin{itemize}
\item thread 0 goes to core 0, which is on socket~0,
\item thread 1 goes to core 1, which is on socket~0,
\item thread 2 goes to core 2, which is on socket~0,
\item and so on, until thread 7 goes to core 7 on socket~0, and
\item thread 8 goes to core 8, which is on socket~1,
\item et cetera.
\end{itemize}
The value \n{OMP_PROC_BIND=close} means that the assignment goes
successively through the available places.
The variable \n{OMP_PROC_BIND} can also be set to \n{spread}, which
spreads the threads over the places.
With
\begin{verbatim}
OMP_PLACES=cores
OMP_PROC_BIND=spread
\end{verbatim}
you find that
\begin{itemize}
\item thread 0 goes to core 0, which is on socket~0,
\item thread 1 goes to core 8, which is on socket~1,
\item thread 2 goes to core 1, which is on socket~0,
\item thread 3 goes to core 9, which is on socket~1,
\item and so on, until thread 14 goes to core 7 on socket~0, and
\item thread 15 goes to core 15, which is on socket~1.
\end{itemize}

So you see that \n{OMP_PLACES=cores} and \n{OMP_PROC_BIND=spread} very
similar to \n{OMP_PLACES=sockets}. The difference is that the latter
choice does not bind a thread to a specific core, so the operating
system can move threads about, and it can put more than one thread on
the same core, even if there is another core still unused.

The value \n{OMP_PROC_BIND=master} puts the threads in the same place
as the master of the team. This is convenient if you create teams
recursively. In that case you would use the \indexclause{proc\_bind}
clause rather than the environment variable, set to \n{spread} for the
initial team, and to \indexpragma{master} for the recursively created team.

\Level 1 {Effects of thread binding}
\label{sec:omp:bindeffect}

Let's consider two example program. First we consider the program for
computing~$\pi$, which is purely compute-bound.

\begin{tabular}{rrrr}
  \toprule
  \#threads&\n{close/cores}&\n{spread/sockets}&\n{spread/cores}\\
  \midrule
   1& 0.359& 0.354& 0.353\\
   2& 0.177& 0.177& 0.177\\
   4& 0.088& 0.088& 0.088\\
   6& 0.059& 0.059& 0.059\\
   8& 0.044& 0.044& 0.044\\
  12& 0.029& 0.045& 0.029\\
  16& 0.022& 0.050& 0.022\\
  \bottomrule
\end{tabular}

We see pretty much perfect speedup for the \n{OMP_PLACES=cores}
strategy; with \n{OMP_PLACES=sockets} we probably get occasional
collisions where two threads wind up on the same core.

Next we take a program for computing the time evolution of the
\indexterm{heat equation}:
\[ t=0,1,2,\ldots\colon \forall_i\colon
x^{(t+1)}_i = 2x^{(t)}_i-x^{(t)}_{i-1}-x^{(t)}_{i+1}
\]
This is a bandwidth-bound operation because the amount of computation
per data item is low.

\begin{tabular}{rrrr}
  \toprule
  \#threads&\n{close/cores}&\n{spread/sockets}&\n{spread/cores}\\
  \midrule
   1& 2.88& 2.89& 2.88\\
   2& 1.71& 1.41& 1.42\\
   4& 1.11& 0.74& 0.74\\
   6& 1.09& 0.57& 0.57\\
   8& 1.12& 0.57& 0.53\\
  12& 0.72& 0.53& 0.52\\
  16& 0.52& 0.61& 0.53\\
  \bottomrule
\end{tabular}

Again we see that \n{OMP_PLACES=sockets} gives worse performance for
high core counts,
probably because of threads winding up on the same core.
The thing to observe in this example is that with 6~or~8 cores the
\n{OMP_PROC_BIND=spread} strategy gives twice the performance of
\n{OMP_PROC_BIND=close}.

The reason for this is that a single socket
does not have enough bandwidth for all eight cores on the
socket. Therefore, dividing the eight threads over two sockets gives
each thread a higher available bandwidth than putting all threads on
one socket.

\Level 1 {Place definition}

There are three predefined values for the \indexompshow{OMP_PLACES}
variable: \n{sockets, cores, threads}. You have already seen the first
two; the \n{threads} value becomes relevant on processors that have
hardware threads. In that case, \n{OMP_PLACES=cores} does not tie a
thread to a specific hardware thread, leading again to possible
collisions as in the above example. Setting \n{OMP_PLACES=threads}
ties each OpenMP thread to a specific hardware thread.

There is also a very general syntax for defining places that uses a
\begin{verbatim}
  location:number:stride
\end{verbatim}
syntax. Examples:
\begin{itemize}
\item
\begin{verbatim}
OMP_PLACES="{0:8:1},{8:8:1}"
\end{verbatim}
  is equivalent to
  \n{sockets} on a two-socket design with eight cores per socket: it
  defines two places, each having eight consecutive cores. The threads
  are then places alternating between the two places, but not further
  specified inside the place.
\item The setting \n{cores} is equivalent to
\begin{verbatim}
OMP_PLACES="{0},{1},{2},...,{15}"
\end{verbatim}
\item On a four-socket design, the specification
\begin{verbatim}
OMP_PLACES="{0:4:8}:4:1"
\end{verbatim}
  states that the place \n{0,8,16,24} needs to be repeated four times,
  with a stride of one. In other words,  thread~0 winds up on
  core~0 of some socket, the thread~1 winds up on core~1 of some
  socket, et cetera.
\end{itemize}

\Level 1 {Binding possibilities}

Values for \indexompshow{OMP_PROC_BIND} are: \n{false, true, master, close, spread}.
\begin{itemize}
\item false: set no binding \item true: lock threads to a core \item
  master: collocate threads with the master thread \item close: place
  threads close to the master in the places list \item spread: spread
  out threads as much as possible
\end{itemize}

This effect can be made local by
giving the \indexclause{proc\_bind} clause in the
\indexpragma{parallel} directive.

A safe default setting is
\begin{verbatim}
export OMP_PROC_BIND=true
\end{verbatim}
which prevents the operating system from
\indextermsub{migrating a}{thread}. This prevents many scaling problems.

Good examples of \emph{thread placement} on the
\emph{Intel Knight's Landing}%
\index{Intel!Knight's Landing!thread placement}:
\url{https://software.intel.com/en-us/articles/process-and-thread-affinity-for-intel-xeon-phi-processors-x200}

As an example, consider a code where two threads write to a shared
location.
%
\cverbatimsnippet[examples/omp/c/sharing.c]{shareboth}
%
There is now a big difference in runtime depending on how close the
threads are. We test this on a processor with both cores and
hyperthreads. First we bind the OpenMP threads to the cores:
\begin{verbatim}
OMP_NUM_THREADS=2 OMP_PLACES=cores OMP_PROC_BIND=close ./sharing
run time = 4752.231836usec
sum = 80000000.0
\end{verbatim}
Next we force the OpenMP threads to bind to hyperthreads inside one core:
\begin{verbatim}
OMP_PLACES=threads OMP_PROC_BIND=close ./sharing
run time = 941.970110usec
sum = 80000000.0
\end{verbatim}
Of course in this example the inner loop is pretty much meaningless
and parallelism does not speed up anything:
\begin{verbatim}
OMP_NUM_THREADS=1 OMP_PLACES=cores OMP_PROC_BIND=close ./sharing
run time = 806.669950usec
sum = 80000000.0
\end{verbatim}
However, we see that the two-thread result is almost as fast, meaning
that there is very little parallelization overhead.

\index{thread!affinity|)}

\Level 0 {First-touch}
\label{sec:first-touch}

The affinity issue shows up in the \indexterm{first-touch}
phenomemon.

A little background knowledge. Memory is organized in
\indextermbus{memory}{page}s\index{page, memory|see{memory, page}},
and what we think of as `addresses' really
\indextermsub{virtual}{address}es,
mapped to \indextermsub{physical}{address}es,
through a \indextermbus{page}{table}.

This means that data in your program can be anywhere in physical memory.
In particular, on a \indextermsub{dual}{socket} node,
the memory can be mapped to either of the sockets.

The next thing to know is that
memory allocated with \indextermtt{malloc} and like
routines is not immediately mapped; that only happens when data is
written to it. In light of this, consider the following OpenMP code:
\begin{lstlisting}
double *x = (double*) malloc(N*sizeof(double));

for (i=0; i<N; i++)
  x[i] = 0;

#pragma omp parallel for
for (i=0; i<N; i++)
  .... something with x[i] ...
\end{lstlisting}
Since the initialization loop is not parallel it is executed by the
master thread, making all the memory associated with the socket of
that thread. Subsequent access by the other socket will then access
data from memory not attached to that socket.

Let's consider an example. We make the initialization
parallel subject to an option:
%
\cverbatimsnippet{heatinitfirst}

If the initialization is not parallel, the array will be mapped
to the socket of the master thread; if it is parallel,
it may be mapped to different sockets, depending on where the threads run.

As a simple application
we run a heat equation, which is parallel,
though not embarassingly so:
%
\cverbatimsnippet{heatmethodrun}

On the \indextermbus{TACC}{Frontera} machine, with dual 28-core
\indextermbus{Intel}{Cascade Lake} processors,
we use the following settings:

\begin{verbatim}
export OMP_PLACES=cores
export OMP_PROC_BIND=close
# no parallel initialization
make heat && OMP_NUM_THREADS=56 ./heat
# yes parallel initialization
make heat && OMP_NUM_THREADS=56 ./heat 1
\end{verbatim}

This gives us a remarkable difference in runtime:
\begin{itemize}
\item
  Sequential init: avg=2.089, stddev=0.1083
\item
  Parallel init: avg=1.006, stddev=0.0216
\end{itemize}
This large difference will be mitigated
for algorithms with higher arithmetic intensity.

\begin{exercise}
  How do the OpenMP dynamic schedules relate to this issue?
\end{exercise}

\Level 1 {C++}

\lstset{language=C++}

The problem with realizing first-touch in \emph{C++}%
\index{C++!first-touch|see{first-touch, in C++}}%
\index{first-touch!in C++}
is that \lstinline+std::vector+ fills its allocation with default values.
This is known as `value-initialization', and it makes
\begin{lstlisting}
vector<double> x(N);
\end{lstlisting}
equivalent to the non-parallel allocation and initialization above.

Here is a solution.
\begin{cppnote}{Uninitialized containers}
  \label{cpp:uninitial}
  We make a template for uninitialized types:
  %
  \cxxverbatimsnippet{cppuninitial}

  so that we can create vectors that behave normally:
  %
  \cxxverbatimsnippet{cppuninitialvec}
\end{cppnote}

Running the code with the regular definition of a vector,
and the above modification,
reproduces the runtimes of the C~variant above.

Another option is to wrap memory allocated with \lstinline{new}
in a \lstinline+unique_ptr+:
%
\cxxverbatimsnippet{newuninitial}

Note that this gives fairly elegant code,
since square bracket indexing is overloaded for \lstinline+unique_ptr+.
The only disadvantage is that we can not query the \lstinline{size}
of these arrays. Or do bound checking with \lstinline{at},
but in high performance contexts that is usually not appropriate anyway.

\lstset{language=C}

\begin{comment}
\begin{exercise}
  \label{ex:first-touch}
  Finish the following fragment and run it with first all the cores of
  one socket, then all cores of both sockets. (If you know how to do
  explicit placement, you can also try fewer cores.)
\begin{lstlisting}
  for (int i=0; i<nlocal+2; i++)
    in[i] = 1.;
  for (int i=0; i<nlocal; i++)
    out[i] = 0.;

  for (int step=0; step<nsteps; step++) {
#pragma omp parallel for schedule(static)
    for (int i=0; i<nlocal; i++) {
      out[i] = ( in[i]+in[i+1]+in[i+2] )/3.;
    }
#pragma omp parallel for schedule(static)
    for (int i=0; i<nlocal; i++)
      in[i+1] = out[i];
    in[0] = 0; in[nlocal+1] = 1;
  }
\end{lstlisting}
\end{exercise}
\end{comment}

\Level 1 {Remarks}

You could move pages with \indextermtt{move_pages}.

By regarding affinity,
in effect you are adopting an \ac{SPMD} style of programming.
You could make this explicit by having each thread allocate its part
of the arrays separately, and storing a private pointer as
\indexpragma{threadprivate}~\cite{Liu:2003:OMP-SPMD}. However, this
makes it impossible for threads to access each other's parts of the
distributed array, so this is only suitable for
total \indextermsub{data}{parallel} or
\indextermsub{embarrassingly}{parallel} applications.

\begin{comment}
[c202-002 c:1] export OMP_PLACES=cores
[c202-002 c:2] export OMP_PROC_BIND=close
[c202-002 c:3] make heat && OMP_NUM_THREADS=56 ./heat
make: `heat' is up to date.
Time=  2.0048 #ops=2.000e+09 on 56 threads
[c202-002 c:4] make heat && OMP_NUM_THREADS=56 ./heat
make: `heat' is up to date.
Time=  2.2479 #ops=2.000e+09 on 56 threads
[c202-002 c:5] make heat && OMP_NUM_THREADS=56 ./heat
make: `heat' is up to date.
Time=  1.9717 #ops=2.000e+09 on 56 threads
[c202-002 c:6] make heat && OMP_NUM_THREADS=56 ./heat
make: `heat' is up to date.
Time=  2.1107 #ops=2.000e+09 on 56 threads
[c202-002 c:7] make heat && OMP_NUM_THREADS=56 ./heat
make: `heat' is up to date.
Time=  2.1087 #ops=2.000e+09 on 56 threads
[c202-002 c:8] make heat && OMP_NUM_THREADS=56 ./heat 1
make: `heat' is up to date.
Parallel init
Time=  1.0420 #ops=2.000e+09 on 56 threads
[c202-002 c:9] make heat && OMP_NUM_THREADS=56 ./heat 1
make: `heat' is up to date.
Parallel init
Time=  0.9962 #ops=2.000e+09 on 56 threads
[c202-002 c:10] make heat && OMP_NUM_THREADS=56 ./heat 1
make: `heat' is up to date.
Parallel init
Time=  1.0077 #ops=2.000e+09 on 56 threads
[c202-002 c:11] make heat && OMP_NUM_THREADS=56 ./heat 1
make: `heat' is up to date.
Parallel init
Time=  0.9939 #ops=2.000e+09 on 56 threads
[c202-002 c:12] make heat && OMP_NUM_THREADS=56 ./heat 1
make: `heat' is up to date.
Parallel init
Time=  0.9880 #ops=2.000e+09 on 56 threads
\end{comment}

\Level 0 {Affinity control outside OpenMP}

There are various utilities to control process and thread placement.

Process placement can be controlled on the Operating system level by
\indextermttdef{numactl}
\begin{tacc}
(the TACC utility \indextermttdef{tacc_affinity} is a wrapper around this)
\end{tacc}
on Linux (also \indextermtt{taskset}); Windows
\indextermtt{start/affinity}.

Corresponding system calls: \indextermtt{pbing} on Solaris,
\indextermtt{sched_setaffinity} on Linux,
\indextermtt{SetThreadAffinityMask} on Windows.

Corresponding environment variables: \indextermtt{SUNW_MP_PROCBIND} on
Solaris, \indextermtt{KMP_AFFINITY} on Intel.

The \emph{Intel compiler}\index{Intel!compiler!thread affinity} has an
environment variable for affinity control:
\begin{verbatim}
export KMP_AFFINITY=verbose,scatter
\end{verbatim}
values: \n{none,scatter,compact}

For \emph{gcc}\index{gcc!thread affinity}:
\begin{verbatim}
export GOMP_CPU_AFFINITY=0,8,1,9
\end{verbatim}

For the \indextermbus{Sun}{compiler}:
\begin{verbatim}
SUNW_MP_PROCBIND
\end{verbatim}

