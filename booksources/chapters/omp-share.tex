% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2024
%%%%
%%%% omp-share.tex : work sharing
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The declaration of a \indexterm{parallel region} establishes a team of
threads. This offers the possibility of parallelism, but to actually
get meaningful parallel activity you need something more.
OpenMP uses the concept of a \indexterm{work sharing
construct}: a way of dividing parallelizable work over a team of threads.

You have already seen loop parallelism
as a way of distributing parallel work
in chapter~\ref{ch:omp-loop}.
We will now discuss other work sharing constructs.

\Level 0 {Work sharing constructs}
\label{sec:work-sharing}

The work sharing constructs are:
\begin{itemize}
\item \texttt{for} (for~C) or
  \texttt{do} (for Fortran): The threads divide up the loop iterations among
  themselves; see~\ref{sec:omp-for}.
\item \texttt{sections}: The threads divide a fixed number of sections
  between themselves; see section~\ref{sec:omp-sections}.
\item \texttt{single} The section is executed by a single thread;
  section~\ref{sec:omp-single}.
\item \texttt{task}: See chapter~\ref{sec:omp:task}.
\item \texttt{workshare}. This can parallelize Fortran array syntax;
  section~\ref{sec:fortran-workshare}.
\end{itemize}

\Level 0 {Sections}
\label{sec:omp-sections}

A parallel loop is an example of independent work units that are numbered.
If you have a pre-determined number of independent work units, 
the \indexomppragma{sections} is more appropriate. In a \lstinline[language=omp]{sections} construct
can be any number of \indexomppragma{section} constructs. These need to be
independent, and they can be execute by any available thread in the current team,
including having multiple sections done by the same thread.
\begin{lstlisting}[language=omp]
#pragma omp sections
{
  #pragma omp section
  // one calculation
  #pragma omp section
  // another calculation
}
\end{lstlisting}

This construct can be used to divide large blocks of independent work.
Suppose that in the following line, both \n{f(x)} and \n{g(x)}
are big calculations:
\begin{lstlisting}[language=omp]
  y = f(x) + g(x) + h(x)
\end{lstlisting}
You could then write
\cverbatimsnippet{ompsections1}
Instead of using two temporaries, you could also use a critical
section; see section~\ref{sec:critical}.  However, the best solution
is have a \lstinline[language=omp]{reduction} clause on the \lstinline[language=omp]{parallel sections} directive.
You could then write
\cverbatimsnippet{ompsections2}

\Level 0 {Single thread execution}
\label{sec:omp-single}

OpenMP has two mechanisms for letting a code section be executed by only
a single thread.
(Note: that is different from critical section which are executed by a single
thread \textsl{at a time}.)
The \indexomppragma{single} directive is to be used for sections
that are part of the control flow, since it has an implicit concluding barrier.
The \indexomppragma{master} and \indexomppragma{masked} directives are similar,
but assign the execution to the primary thread, and have no concluding barrier.

\Level 1 {Single}

The \indexomppragmadef{single} pragma
limits the execution of a block to a single thread. 
This can for instance be used to print tracing information
or doing \emph{I/O}\index{I/O!in OpenMP} operations.
\begin{lstlisting}[language=omp]
#pragma omp parallel
{
  #pragma omp single
  printf("We are starting this section!\n");
  // parallel stuff
}
\end{lstlisting}
Another use of \lstinline[language=omp]{single} is to perform initializations
in a parallel region:
\begin{lstlisting}[language=omp]
int a;
#pragma omp parallel
{
  #pragma omp single
    a = f(); // some computation
  #pragma omp sections
    // various different computations using a
}
\end{lstlisting}

The point of the single directive in this last example is that the
computation needs to be done only once, because of the shared memory.
Since it's a work sharing construct there is an \emph{implicit
  barrier}\index[omp]{implicit barrier!after single directive} after it,
which guarantees that all threads have the correct value in their
local memory (see section~\ref{sec:omp:flush}).

\begin{exercise}
  \label{ex:omp-single-mpi}
  What is the difference between this approach and how the same
  computation would be parallelized in MPI?
\end{exercise}

\Level 1 {Masked/master}

The \indexomppragmadef{masked} and \indexomppragmadef{master} directives
also enforces execution on a single thread,
specifically the primary thread of the team.
This is not a work sharing construct, and therefore
does not have the synchronization through the implicit barrier.

\begin{remark}
  The \indexomppragma{masked} directive is new in \ompstandard{5.1}.
  The \indexomppragma{master} directive is deprecated
  as of \ompstandard{5.2}.
\end{remark}

\begin{exercise}
  Modify the above code to read:
\begin{lstlisting}[language=omp]
int a;
#pragma omp parallel
{
  #pragma omp master
    a = f(); // some computation
  #pragma omp sections
    // various different computations using a
}
\end{lstlisting}
  This code is no longer correct. Explain.
\end{exercise}

The \indexomppragma{masked} directive has a \indexclause{filter} clause,
that can be used to select other threads than the primary:
\begin{lstlisting}[language=omp]
#pragma omp masked filter(2) // thread #2
\end{lstlisting}

\Level 1 {More}

Above we motivated the \lstinline[language=omp]{single} directive as a way of initializing
shared variables. It is also possible to use \lstinline[language=omp]{single} to initialize
private variables. In that case you add the \indexclausedef{copyprivate}
clause. This is a good solution if setting the variable takes~I/O.

\begin{exercise}
  Give two other ways to initialize a private variable, with all
  threads receiving the same value. Can you give scenarios where each
  of the three strategies would be preferable?
\end{exercise}

\Level 0 {Fortran array syntax parallelization}
\label{sec:fortran-workshare}

The \lstinline[language=omp]{parallel do} directive is used to parallelize loops,
and this applies to both C and Fortran. However, Fortran also
has implied loops in its \emph{array syntax}\index{Fortran!array syntax}.
To parallelize array syntax you can use the \indexomppragma{workshare}
directive.

The \indexomppragma{workshare} directive exists only in Fortran.
It can be used to parallelize
the implied loops in \emph{array syntax}\index{Fortran!array syntax},
as well as  \emph{forall}\index{Fortran!forall loops} loops.

We compare two version of $C\leftarrow C+A\times B$
(where all operations are elementwise),
running on \indextermbus{TACC}{Frontera}
up to 56~cores.

\begin{multicols}{2}
  Workshare based:
  \fverbatimsnippet{wsmatmul}
  \columnbreak
  SIMD'ized loop
  \fverbatimsnippet{sdmatmul}
\end{multicols}

With results:

\begin{verbatim}
SIMD times     :
0.07115 0.04053 0.02498 0.01609 0.01210 0.01247 0.01765 0.02689
Speedup:
 1 1.75549 2.84828 4.422 5.88017 5.70569 4.03116 2.64597

Workshare times:
0.06188 0.03186 0.01625 0.00867 0.00619 0.00379 0.00354 0.00373
Speedup:
 1 1.94225 3.808 7.13725 9.99677 16.3272 17.4802 16.5898  
\end{verbatim}
