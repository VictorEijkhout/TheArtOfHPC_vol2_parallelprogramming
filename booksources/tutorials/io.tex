% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2022
%%%%
%%%% io.tex : parallel I/O
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Parallel I/O is a tricky subject. You can try to let all processors
jointly write one file, or to write a file per process and combine
them later. With the standard mechanisms of your programming language
there are the following considerations:
\begin{itemize}
\item On clusters where the processes have individual file systems,
  the only way to write a single file is to let it be generated by a
  single processor.
\item Writing one file per process is easy to do, but
  \begin{itemize}
  \item You need a post-processing script;
  \item if the files are not on a shared file system (such as
    \indexterm{Lustre}), it takes additional effort to bring them
    together;
  \item if the files \emph{are} on a shared file system, writing many
    files may be a burden on the metadata server.
  \end{itemize}
\item On a shared file system it is possible for all files to open the
  same file and set the file pointer individually. This can be
  difficult if the amount of data per process is not uniform.
\end{itemize}
Illustrating the last point:
%
\cverbatimsnippet[code/mpi/c/pseek.c]{pseek}

MPI also has its own portable I/O: \indextermbus{MPI}{I/O}, for which
see chapter~\ref{ch:mpi-io}.

Alternatively, one could use a library such as \indexterm{hdf5};
see~\CARPref{tut:hdf5}.

For a great discussion see~\cite{Mendez:ParallelIOpage},
from which figures here are taken.

\Level 0 {Use sequential I/O}

MPI processes can do anything a regular process can,
including opening a file.
This is the simplest form of parallel I/O:
every MPI process opens its own file.
To prevent write collisions,
\begin{itemize}
\item you use \indexmpishow{MPI_Comm_rank} to generate a unique file name, or
\item you use a local file system, typically \n{/tmp}, that is unique
  per process, or at least per the group of processes on a node.
\end{itemize}

For reading it is actually possible for all processes to open the same file,
but for reading this is not really feasible. Hence the unique files.

\Level 0 {MPI I/O}

In chapter~\ref{ch:mpi-io} we discussed MPI I/O.
This is a way for all processes on a communicator to open a single file,
and write to it in a coordinated fashion.
This has the big advantage that the end result is an ordinary Unix file.

\Level 0 {Higher level libraries}

Libraries such as \indexterm{NetCDF} or \indexterm{HDF5}
offer advantages over MPI I/O:
\begin{itemize}
\item Files can be OS-independent, removing worries such
  as about \indexterm{little-endian} storage.
\item Files are self-documenting: they contain the metadata describing their contents.
\end{itemize}


