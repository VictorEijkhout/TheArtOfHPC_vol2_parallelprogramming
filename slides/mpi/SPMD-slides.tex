% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-2025
%%%%
%%%% SPMD-slides.tex : slides about MPI's SPMD mode
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Overview}
  In this section you will learn how to think about parallelism in
  MPI.

  Commands learned:
  \begin{itemize}
  \item
    \indexmpishow{MPI_Init}, \indexmpishow{MPI_Finalize},
  \item 
    \indexmpishow{MPI_Comm_size}, \indexmpishow{MPI_Comm_rank}
  \item 
    \indexmpishow{MPI_Get_processor_name}, 
  \end{itemize}
\end{numberedframe}

\Level 2 {Practicalities}

\begin{tacc}
\begin{numberedframe}{Lab setup}
  \begin{itemize}
  \item Clone the repository\\
    \url{https://github.com/VictorEijkhout/TheArtOfHPC_vol2_parallelprogramming}
  \item Directory: \n{exercises-mpi-c} or \n{cxx} or \n{f} or \n{f08}
    or \n{p} or \n{mpl}
  \item Open a terminal window on a TACC cluster.
  \item Type \n{idev -N 2 -n 10 -t 2:0:0 } which gives
    you an interactive session of 2~nodes, 10~cores, for the next
    2~hours.
  \item Type \n{make exercisename} to compile it
  \item Run with \n{ibrun} or \n{mpiexec} (see above)
  \end{itemize}
\end{numberedframe}
\end{tacc}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{Python}
 Python: setup once per session
\begin{verbatim}
module load python3
\end{verbatim}
No compilation needed. Run:
\begin{verbatim}
ibrun python3 yourprogram.py
\end{verbatim}
\end{numberedframe}
\end{python}

\begin{numberedframe}{Compiling}
  MPI compilers are usually called \indextermtt{mpicc},
  \indextermtt{mpif90}, \indextermtt{mpicxx}.

  These are not separate compilers,
  but scripts around the regular C/Fortran compiler. You can use all
  the usual flags.
\begin{verbatim}
$ mpicc -show
icc -I/intel/include/stuff -L/intel/lib/stuff -Wwarnings # et cetera
\end{verbatim}
\begin{full}
  (For CMake see slide~\ref{sl:mpi-cmake}.)
\end{full}
\end{numberedframe}

\begin{numberedframe}{Running}
\begin{tacc}
  Running your program at TACC:
\begin{verbatim}
#SBATCH -N 4
#SBATCH -n 200
ibrun yourprog
\end{verbatim}
  the number of processes is determined by SLURM.
\end{tacc}

Outside TACC:
\begin{verbatim}
mpiexec -n 4 hostfile ... yourprogram arguments
mpirun -np 4 hostfile ... yourprogram arguments
\end{verbatim}
\end{numberedframe}

\begin{exerciseframe}[hello]
  \input ex:hello1

  \begin{enumerate}
  \item In the directories \n{exercises-mpi-xxx} do 
    \n{make hello} to compile;
  \item do \n{ibrun hello} to execute.
  \end{enumerate}
\end{exerciseframe}

\begin{numberedframe}{In a picture}
  \moveleft .3in \hbox\bgroup
  \includegraphics[scale=.32]{hello-parallel}
  \egroup
\end{numberedframe}

\Level 2 {The MPI worldview: SPMD}

\begin{numberedframe}{SPMD}
  The basic model of MPI is\\
  `Single Program Multiple Data':\\
  each process is an instance of the same program.

  Symmetry: There is no `master process', all processes are equal,
  start and end
  at the same time.

  Communication calls do not see the cluster structure:\\
  data sending/receiving is the same for all neighbors.
\end{numberedframe}

\begin{numberedframe}{Computers when MPI was designed}
  \includegraphics[scale=.1]{mpi-node1.png}

  One processor and one  process per node;\\
  all communication goes through the network.

  $\Rightarrow$ process model, no data sharing.
\end{numberedframe}

\begin{numberedframe}{Pure MPI}
  \includegraphics[scale=.1]{mpi-node2.png}

  A node has multiple sockets, each with multiple cores.\\
  Pure MPI puts a process on each core: pretend shared memory doesn't exist.
\end{numberedframe}

\begin{numberedframe}{Quad socket node}
  \includegraphics[scale=.8]{ranger-node-small}
\end{numberedframe}

\begin{numberedframe}{Hybrid programming}
  \includegraphics[scale=.1]{mpi-node3.png}

  Hybrid programming puts a process per node or per socket;\\
  further parallelism comes from threading.\\
  Not in this course\ldots
\end{numberedframe}

\begin{numberedframe}{Terminology}
  `Processor' is ambiguous: is that a chip or one independent
  instruction processing unit?
  \begin{itemize}
  \item Socket: the processor chip
  \item Processor: we don't use that word
  \item Core: one instruction-stream processing unit
  \item Process: preferred terminology in talking about MPI.
  \end{itemize}  
\end{numberedframe}

\begin{numberedframe}{Do I need a supercomputer?}
  \begin{itemize}
  \item With \n{mpiexec} and such, you start a bunch of processes that
    execute your MPI program.
  \item Does that mean that you need a cluster or a big multicore?
  \item No! You can start a large number of MPI processes, even on
    your laptop. The OS will use `time slicing'.
  \item Of course it will not be very efficient\ldots
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Installing your own MPI}
  It is convenient to do MPI development on your laptop/desktop.
  \begin{itemize}
  \item Use a package manager\\
    Apple: brew or macports\\
    Linux: yum, aptget,~\ldots\\
    Windows: I'll have to get back to you on that
  \item \ldots~or download and compile from source \n{mpich.org}
  \end{itemize}
\end{numberedframe}

\Level 2 {We start learning MPI!}

\begin{numberedframe}{MPI Init / Finalize}
 These calls need to be around the MPI part of your code:
\lstset{language=C}
\begin{lstlisting}
MPI_Init(&argc,&argv); // zeros allowed
// your code
MPI_Finalize();  
\end{lstlisting}
This is not a `parallel region'.\\
Only internal library initialization:\\
allocate buffers, discover network,~\ldots
\end{numberedframe}

\begin{mpl}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{MPL init/finalize}
    No explicit init/finalize:
    \begin{itemize}
    \item init is done by the first command that needs it
    \item finalize in some destructor.
    \end{itemize}
\end{numberedframe}
\end{mpl}

\begin{fortran}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{Init Finalize, Fortran}
No possibility for commandline arguments:
\lstset{language=Fortran}
\begin{lstlisting}
call MPI_Init()     ! F08 style
! your code
call MPI_Finalize()

call MPI_Init(ierr) ! F90 style
! your code
call MPI_Finalize(ierr)
\end{lstlisting}  
\end{numberedframe}
\end{fortran}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{Python init/finalize}
    Done by the import~/ at end of the program.
\begin{verbatim}
from mpi4py import MPI
\end{verbatim}
\end{numberedframe}
\end{python}

\begin{exerciseframe}[hello]
  \input ex:hello2
\end{exerciseframe}

\Level 2 {About library calls and bindings}

\begin{numberedframe}{Bindings}
  The standard defines interfaces to MPI
  from C and Fortran.\\
  These look very similar;
  sometimes we will only show the C variant.

  MPI can also be used from C++ and Python
\end{numberedframe}

\begin{numberedframe}{MPI headers: C}
\label{sl:mpi-header-c}
You need an include file:
\begin{verbatim}
#include "mpi.h"
\end{verbatim}
This defines all routines and constants.
\end{numberedframe}

\begin{mpl}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{C++ bindings, MPL}
  MPI-1 had C++ bindings, by MPI-2 they were deprecated, in MPI-3 they have been removed.
  \begin{itemize}
  \item Easy solution: use the C~bindings unaltered.\\
    This is done in the \n{cxx} exercise directory;  Ugly: very un-OO.
  \item There are private projects for C++ bindings.\\
    In particular
    MPL: \url{https://github.com/rabauke/mpl}\\
    (Exercises in \n{mpl} directory.)\\
    Very modern OO,  Header-only\\
    Not a full MPI implementation: (I/O and one-sided mostly missing)
  \end{itemize}
\end{numberedframe}
\begin{numberedframe}{Use of MPL}
    In your program:
    \begin{lstlisting}
      #include <mpl/mpl.hpp>
  \end{lstlisting}
    Compiling:
\begin{verbatim}
  mpicxx -o prog sources.cxx -I${TACC_MPL_INC}
\end{verbatim}
TACC: \texttt{module load mpl}
\end{numberedframe}
\end{mpl}

\begin{fortran}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{MPI headers: Fortran}
    \label{sl:mpi-header-f}
    Fortran bindings are declared in the standard.
    \begin{itemize}
    \item Come in two flavors, Fortran90 vs Fortran2008\\
      you will find many examples online in old style
    \item We teach you modern style.
    \end{itemize}
    You need an include file:
\begin{verbatim}
! Modern Fortran2008
use mpi_f08
! Legacy Fortran90
use mpi       
! Deprecated
#include "mpif.h"
\end{verbatim}
\end{numberedframe}
\end{fortran}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{Python bindings}
    \label{sl:mpi-header-p}
    \begin{itemize}
    \item Not part of the standard:\\
      private project by Lisandro Dalcin\\
      Download \url{https://github.com/mpi4py/mpi4py}\\
      Docs: \url{https://mpi4py.readthedocs.io/}
    \item Comes in two variants:\\
      `pythonic' vs efficient
    \end{itemize}
    You need an include file:
\begin{verbatim}
from mpi4py import MPI
\end{verbatim}
You need a python with MPI support\\
at TACC: \texttt{module load python3}
\end{numberedframe}
\end{python}

\Level 2 {Ranks}

\begin{numberedframe}{Process identification}
  \label{sl:comm-world}
  \begin{itemize}
  \item Processes are organized in `communicators'.
  \item For now only the `world' communicator
  \item Each process has a unique `rank' wrt the communicator.
  \end{itemize}
\lstset{language=C}
\begin{lstlisting}
int MPI_Comm_size( MPI_Comm comm, int *nprocs )
int MPI_Comm_rank( MPI_Comm comm, int *procno )
\end{lstlisting}
Lowest number is always zero.

This is a logical view of parallelism: mapping to physical
processors/cores is invisible here.
\end{numberedframe}

\begin{numberedframe}{World communicator}
  \label{sl:mpi-comm-world}
  For now, the communicator will be \indexmpishow{MPI_COMM_WORLD}.

  C:
  \lstset{language=C}
\begin{lstlisting}
MPI_Comm comm = MPI_COMM_WORLD;
\end{lstlisting}

  F:
  \lstset{language=Fortran}
\begin{lstlisting}
Type(MPI_Comm) :: comm = MPI_COMM_WORLD
\end{lstlisting}

  P:
  \lstset{language=Python}
\begin{lstlisting}
from mpi4py import MPI
comm = MPI.COMM_WORLD
\end{lstlisting}

  MPL:
  \lstset{language=C++}
\begin{lstlisting}
    const mpl::communicator &comm_world =
        mpl::environment::comm_world();
\end{lstlisting}
\end{numberedframe}

\protoslide{MPI_Comm_size}
\protoslide{MPI_Comm_rank}

\begin{numberedframe}{About routine signatures: C/C++}
  \label{sec:protos}
Signature:
\lstset{language=C}
\begin{lstlisting}
int MPI_Comm_size(MPI_Comm comm,int *nprocs)
\end{lstlisting}
Use:
\lstset{language=C}
\begin{lstlisting}
MPI_Comm comm = MPI_COMM_WORLD;
int nprocs;
int errorcode;
errorcode = MPI_Comm_size( comm,&nprocs );
\end{lstlisting}
But forget about that error code most of the time:
\begin{lstlisting}
MPI_Comm_size( comm,&nprocs );
\end{lstlisting}
\end{numberedframe}

\begin{mpl}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{About routine signatures: MPL}
  %% \input standardx/size
  Signature:
  \verbatiminput{mplreference/MPI_Comm_rank.tex}
  Use
\begin{lstlisting}
const mpl::communicator &comm_world = mpl::environment::comm_world();
int nprocs, procno;
nprocs = comm_world.size();
\end{lstlisting}
\end{numberedframe}
\end{mpl}

\begin{fortran}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{About routine signatures: Fortran2008}
Signature
\lstset{language=Fortran}
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Type(MPI_Comm), INTENT(IN) :: comm
INTEGER, INTENT(OUT) :: size
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
\end{lstlisting}
Use:
\begin{lstlisting}
Type(MPI_Comm) :: comm = MPI_COMM_WORLD
integer :: size
CALL MPI_Comm_size( comm, size ) ! F2008 style
\end{lstlisting}
\begin{itemize}
\item final parameter optional.
\item \n{MPI_...} types are \n{Type}.
\end{itemize}
\end{numberedframe}

\begin{numberedframe}[f]{About routine signatures: Fortran90}
Signature
\lstset{language=Fortran}
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Integer, Intent(in) :: comm
Integer, Intent(out) :: ierror
\end{lstlisting}
Use:
\begin{lstlisting}
Integer :: comm = MPI_COMM_WORLD
Integer :: size,ierr
CALL MPI_Comm_size( comm, size, ierr ) ! F90 style
\end{lstlisting}
\begin{itemize}
\item Final parameter always error parameter. Do not forget!
\item \n{MPI_...} types are \n{INTEGER}.
\end{itemize}
\end{numberedframe}
\end{fortran}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{About routine signatures: Python}
Signature:
\lstset{language=Python}
\begin{lstlisting}
# object method
MPI.Comm.Send(self, buf, int dest, int tag=0)
# class method
MPI.Request.Waitall(type cls, requests, statuses=None)
\end{lstlisting}
Use:
\begin{lstlisting}
from mpi4py import MPI
comm = MPI.COMM_WORLD
comm.Send(sendbuf,dest=other)
MPI.Request.Waitall(requests)
\end{lstlisting}
Note: most functions are methods of the \lstinline{MPI.Comm} class.\\
(Sometimes of \lstinline{MPI}, sometimes other.)
\end{numberedframe}
\end{python}

\begin{exerciseframe}[commrank]
  \input ex:hello3
\end{exerciseframe}

\begin{exerciseframe}[commrank]
  \input ex:hello4
\end{exerciseframe}

\begin{numberedframe}{Illustration}
  \includegraphics[scale=.35]{rank-parallel}
\end{numberedframe}

\begin{numberedframe}{About errors}
  MPI routines invoke an error handler (slide~\ref{sl:errhandler})\\
  default action: abort
  
  Every routine is defined as returning integer error code
  \begin{itemize}
  \item In C: function result. 
\lstset{language=C++}
\begin{lstlisting}
ierr = MPI_Init(0,0);
if (ierr!=MPI_SUCCESS) /* do something */
\end{lstlisting}
  But really: can often be ignored; is ignored in this course.
\begin{lstlisting}
MPI_Init(0,0);
\end{lstlisting}
  \item In Fortran: as optional (F08 only) parameter.
  \item MPL: throws exceptions
  \item In Python: throwing exception.
  \end{itemize}
  There's not a lot you can do with an error code:\\
  very hard to recover from errors in parallel.\\
  By default code bombs with (hopefully informative) message.
\end{numberedframe}

%% \begin{numberedframe}{Have you been paying attention?}
%%   \input{ex:mpi01rank}
%% \end{numberedframe}

\protoslide{MPI_Get_processor_name}

\begin{exerciseframe}
  \input ex:procname

  Go to \n{examples/mpi/xxx} and do \n{make procname},
  then \n{ibrun procname}
\end{exerciseframe}

\begin{numberedframe}{Processor name}
  Processes (can) run on physically distinct locations.

  \cverbatimsnippet{procname}
\end{numberedframe}

\begin{numberedframe}{In a picture}
  Four processes on two nodes (\n{idev -N 2 -n 4})
  \includegraphics[scale=.3]{node-comm-rank}
\end{numberedframe}

\begin{mpl}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}[l]{Processor name: MPL}
  Processes (can) run on physically distinct locations.

  \cxxsnippetwithoutput{mplprocname}{examples/mpi/mpl}{procname}

  (Why \lstinline{stringstream} instead of plain \lstinline{cout}?)
\end{numberedframe}
\end{mpl}

\begin{fortran}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}[f]{Processor name: Fortran}
  Processes (can) run on physically distinct locations.

  \fverbatimsnippet{procnamef}
\end{numberedframe}
\end{fortran}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}[p]{Processor name: Python}
  Processes (can) run on physically distinct locations.

  \pverbatimsnippet{procnamep}
\end{numberedframe}
\end{python}


%% \protoslide{MPI_Get_processor_name}
%% \begin{mpl}
%%   \mplprotoslide{MPI_Get_processor_name}
%% \end{mpl}

\Level 2 {Your first useful parallel program}

\begin{numberedframe}{Functional Parallelism}
  Parallelism by letting each process do a different thing.

  Example: divide up a search space.

  Each process knows its rank, so it can find its part of the search space.
\end{numberedframe}

\begin{exerciseframe}[prime]
  \input ex:primetest
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:array-ints}
\end{exerciseframe}

\endinput

\begin{numberedframe}\frametitle{}
\begin{lstlisting}
  
\end{lstlisting}
\end{numberedframe}

