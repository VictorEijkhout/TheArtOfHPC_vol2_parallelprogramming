% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-2023
%%%%
%%%% SPMD-slides.tex : slides about MPI's SPMD mode
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Overview}
  In this section you will learn how to think about parallelism in
  MPI.

  Commands learned:
  \begin{itemize}
  \item
    \indexmpishow{MPI_Init}, \indexmpishow{MPI_Finalize},
  \item 
    \indexmpishow{MPI_Comm_size}, \indexmpishow{MPI_Comm_rank}
  \item 
    \indexmpishow{MPI_Get_processor_name}, 
  \end{itemize}
\end{numberedframe}

\sectionframe{The MPI worldview: SPMD}

\begin{numberedframe}{Computers when MPI was designed}
  \includegraphics[scale=.1]{mpi-node1}

  One processor and one  process per node;\\
  all communication goes through the network.
\end{numberedframe}

\begin{numberedframe}{Pure MPI}
  \includegraphics[scale=.1]{mpi-node2}

  A node has multiple sockets, each with multiple cores.\\
  Pure MPI puts a process on each core: pretend shared memory doesn't exist.
\end{numberedframe}

\begin{numberedframe}{Quad socket node}
  \includegraphics[scale=.8]{ranger-node-small}
\end{numberedframe}

\begin{numberedframe}{Hybrid programming}
  \includegraphics[scale=.1]{mpi-node3}

  Hybrid programming puts a process per node or per socket;\\
  further parallelism comes from threading.\\
  Not in this course\ldots
\end{numberedframe}

\begin{numberedframe}{Terminology}
  `Processor' is ambiguous: is that a chip or one independent
  instruction processing unit?
  \begin{itemize}
  \item Socket: the processor chip
  \item Processor: we don't use that word
  \item Core: one instruction-stream processing unit
  \item Process: preferred terminology in talking about MPI.
  \end{itemize}  
\end{numberedframe}

\begin{numberedframe}{SPMD}
  The basic model of MPI is\\
  `Single Program Multiple Data':\\
  each process is an instance of the same program.

  Symmetry: There is no `master process', all processes are equal,
  start and end
  at the same time.

  Communication calls do not see the cluster structure:\\
  data sending/receiving is the same for all neighbors.
\end{numberedframe}

\sectionframe{Practicalities}

\begin{numberedframe}{Compiling and running}
  MPI compilers are usually called \indextermtt{mpicc},
  \indextermtt{mpif90}, \indextermtt{mpicxx}.

  These are not separate compilers,
  but scripts around the regular C/Fortran compiler. You can use all
  the usual flags.
\begin{verbatim}
$ mpicc -show
icc -I/intel/include/stuff -L/intel/lib/stuff -Wwarnings # et cetera
\end{verbatim}

\begin{tacc}
  Running your program at TACC:
\begin{verbatim}
#SBATCH -N 4
#SBATCH -n 200
ibrun yourprog
\end{verbatim}
  the number of processes is determined by SLURM.
\end{tacc}
General case of running code
\begin{verbatim}
mpiexec -n 4 hostfile ... yourprogram arguments
mpirun -np 4 hostfile ... yourprogram arguments
\end{verbatim}
\end{numberedframe}

\begin{numberedframe}{Do I need a supercomputer?}
  \begin{itemize}
  \item With \n{mpiexec} and such, you start a bunch of processes that
    execute your MPI program.
  \item Does that mean that you need a cluster or a big multicore?
  \item No! You can start a large number of MPI processes, even on
    your laptop. The OS will use `time slicing'.
  \item Of course it will not be very efficient\ldots
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Installing your own MPI}
  It is convenient to do MPI development on your laptop/desktop.
  \begin{itemize}
  \item Use a package manager\\
    Apple: brew of macports\\
    Linux: yum, aptget,~\ldots\\
    Windows: I'll have to get back to you on that
  \item \ldots~or download and compile from source \n{mpich.org}
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Cluster setup}
  \small
  Typical cluster:
  \begin{itemize}
  \item Login nodes, where you ssh into; usually shared with 100 (or
    so) other people. You don't run your parallel program there!
  \item Compute nodes: where your job is run. They are often exclusive
    to you: no other users getting in the way of your program.
  \end{itemize}
  Hostfile: the description of where your job runs. Usually generated
  by a \indexterm{job scheduler}.
\end{numberedframe}

\begin{numberedframe}{Interactive run}
  \begin{itemize}
  \item Do not run your programs on a login node.
  \item Acquire compute nodes with \indextermtt{idev} \\
    (other systems: \indextermtt{qsub}\n{ -I})
  \item Caveat: only small short jobs; nodes may not be available.
  \end{itemize}
  \includegraphics[scale=.15]{job_idev}
\end{numberedframe}

\begin{numberedframe}{Batch run}
  \begin{itemize}
  \item Submit batch job with \indextermtt{sbatch} or \indextermtt{qsub}
  \item Your job will be executed~\ldots Real Soon Now.
  \item See userguide for details about queues, sizes, runtimes,~\ldots
  \end{itemize}
  \includegraphics[scale=.15]{job_sbatch}
\end{numberedframe}

\begin{tacc}
\begin{numberedframe}{Lab setup}
    \begin{itemize}
    \item Open a terminal window on a TACC cluster.
    \item Type \n{idev -N 2 -n 10 -t 2:0:0 } which gives
      you an interactive session of 2~nodes, 10~cores, for the next
      2~hours.
    \item (After this course, for serious work, you would write a
      batch script. The idev sessions are strictly limited in time and
      resources.)
    \item See external handouts for reservations, project IDs,
      and location of training materials.
    \item Next slide for how to make and run exercises.
    \end{itemize}
\end{numberedframe}
\end{tacc}

\begin{numberedframe}{How to make exercises}
  \begin{itemize}
  \item Directory: \n{exercises-mpi-c} or \n{cxx} or \n{f} or \n{f08}
    or \n{p} or \n{mpl}
  \item If a slide has a \n{(exercisename)} over it, there will be a
    template program \n{exercisename.c} (or \n{F90} or \n{py}).
  \item Type \n{make exercisename} to compile it
  \item Run with \n{ibrun} or \n{mpiexec} (see above)
  \item Python: setup once per session
\begin{verbatim}
module load python3
\end{verbatim}
No compilation needed. Run:
\begin{verbatim}
ibrun python3 yourprogram.py
\end{verbatim}
\item Add an exercise of your own to the makefile: add the name to
    the \n{EXERCISES}
  \end{itemize}
\end{numberedframe}

\begin{exerciseframe}[hello]
  \input ex:hello1
\end{exerciseframe}

\begin{numberedframe}{In a picture}
  \includegraphics[scale=.35]{hello-parallel}
\end{numberedframe}

\sectionframe{We start learning MPI!}

\begin{numberedframe}{Bindings}
  The standard defines interfaces to MPI
  from C and Fortran.\\
  These look very similar;
  sometimes we will only show the C variant.

  MPI can also be used from C++ and Python
\end{numberedframe}

\begin{numberedframe}{MPI headers: C}
\label{sl:mpi-header-c}
You need an include file:
\begin{verbatim}
#include "mpi.h"
\end{verbatim}
This defines all routines and constants.
\end{numberedframe}

\begin{fortran}
  \addtocounter{slidecount}{-2}
\begin{numberedframe}{Fortran bindings}
    \begin{itemize}
    \item
      In the standard
    \item Come in two flavors, Fortran90 vs Fortran2008\\
      you will find many examples online in old style
    \item We teach you modern style.
    \end{itemize}
\end{numberedframe}

\begin{numberedframe}{MPI headers: Fortran}
    \label{sl:mpi-header-f}
    You need an include file:
\begin{verbatim}
! Modern Fortran2008
use mpi_f08
! Legacy Fortran90
use mpi       
! Deprecated
#include "mpif.h"
\end{verbatim}
\end{numberedframe}
\end{fortran}

\begin{mpl}
  \addtocounter{slidecount}{-2}
\begin{numberedframe}{C++ bindings, MPL}
  MPI-1 had C++ bindings, by MPI-2 they were deprecated, in MPI-3 they have been removed.
  \begin{itemize}
  \item Easy solution: use the C~bindings unaltered.
    \begin{itemize}
    \item This is done in the \n{cxx} exercise directory.
    \item Ugly: very un-OO.
    \end{itemize}
  \item There are private projects for C++ bindings.\\
    In particular
    MPL: \url{https://github.com/rabauke/mpl}
    \begin{itemize}
    \item Very modern OO.
    \item Header-only
    \item Caution: not a full MPI implementation\\
      (I/O and one-sided mostly missing)
    \item Exercises in \n{mpl} directory.
    \end{itemize}
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{MPL header}
    In your program:
    \begin{lstlisting}
      #include <mpl/mpl.hpp>
    \end{lstlisting}
    Compiling:
\begin{verbatim}
  mpicxx -o prog sources.cxx -I${TACC_MPL_INC}
\end{verbatim}
TACC: \texttt{module load mpl}
\end{numberedframe}
\end{mpl}

\begin{python}
  \addtocounter{slidecount}{-2}
\begin{numberedframe}{Python bindings}
    \begin{itemize}
    \item Not part of the standard:\\
      private project by Lisando Dalcin\\
      Download \url{https://github.com/mpi4py/mpi4py}\\
      Docs: \url{https://mpi4py.readthedocs.io/}
    \item Comes in two variants:\\
      `pythonic' vs efficient
    \end{itemize}
\end{numberedframe}

\begin{numberedframe}{MPI headers: Python}
    \label{sl:mpi-header-p}
    You need an include file:
\begin{verbatim}
from mpi4py import MPI
\end{verbatim}
You need a python with MPI support\\
at TACC: \texttt{module load python3}
\end{numberedframe}
\end{python}

\begin{numberedframe}{MPI Init / Finalize}
Then put these calls around your code:
\lstset{language=C}
\begin{lstlisting}
MPI_Init(&argc,&argv); // zeros allowed
// your code
MPI_Finalize();  
\end{lstlisting}
This is not a `parallel region':\\
only internal library initialization:\\
allocate buffers, discover network,~\ldots
\end{numberedframe}

\begin{fortran}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{Init Finalize, Fortran}
No possibility for commandline arguments:
\lstset{language=Fortran}
\begin{lstlisting}
call MPI_Init()     ! F08 style
! your code
call MPI_Finalize()

call MPI_Init(ierr) ! F90 style
! your code
call MPI_Finalize(ierr)
\end{lstlisting}  
\end{numberedframe}
\end{fortran}

\begin{mpl}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{MPL init/finalize}
    No explicit init/finalize:
    \begin{itemize}
    \item init is done by the first command that needs it
    \item finalize in some destructor.
    \end{itemize}
\end{numberedframe}
\end{mpl}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{Python init/finalize}
    Done by the import~/ at end of the program.
\end{numberedframe}
\end{python}

\begin{numberedframe}{About errors}
  MPI routines invoke an error handler (slide~\ref{sl:errhandler})\\
  default action: abort
  
  Every routine is defined as returning integer error code
  \begin{itemize}
  \item In C: function result. 
\lstset{language=C++}
\begin{lstlisting}
ierr = MPI_Init(0,0);
if (ierr!=MPI_SUCCESS) /* do something */
\end{lstlisting}
  But really: can often be ignored; is ignored in this course.
\begin{lstlisting}
MPI_Init(0,0);
\end{lstlisting}
  \item In Fortran: as optional (F08 only) parameter.
  \item In Python: throwing exception.
  \end{itemize}
  There's not a lot you can do with an error code:\\
  very hard to recover from errors in parallel.\\
  By default code bombs with (hopefully informative) message.
\end{numberedframe}

\begin{exerciseframe}[hello]
  \input ex:hello2

  Python, MPL: include magic line from slide~\ref{sl:mpi-comm-world}.
\end{exerciseframe}

\begin{numberedframe}{Process identification}
  \label{sl:comm-world}
  \begin{itemize}
  \item Processors are organized in `communicators'.
  \item For now only the `world' communicator
    (slide~\ref{sl:comm-world})
  \item Each process has a `rank' wrt the communicator.
  \end{itemize}

\lstset{language=C}
\begin{lstlisting}
int MPI_Comm_size( MPI_Comm comm, int *nprocs )
int MPI_Comm_rank( MPI_Comm comm, int *procno )
\end{lstlisting}
Lowest number is always zero.

This is a logical view of parallelism: mapping to physical
processors/cores is invisible here.
\end{numberedframe}

\begin{numberedframe}{Communicators}
  \label{sl:mpi-comm-world}
  For now, the communicator will be \indexmpishow{MPI_COMM_WORLD}.

  C:
  \lstset{language=C}
  \begin{lstlisting}
    MPI_Comm comm = MPI_COMM_WORLD;
  \end{lstlisting}

  F:
  \lstset{language=Fortran}
  \begin{lstlisting}
    Type(MPI_Comm) :: comm = MPI_COMM_WORLD
  \end{lstlisting}

  P:
  \lstset{language=Python}
  \begin{lstlisting}
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
  \end{lstlisting}

  MPL:
  \lstset{language=C++}
  \begin{lstlisting}
    const mpl::communicator &comm_world =
        mpl::environment::comm_world();
  \end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{Illustration}
  \includegraphics[scale=.35]{rank-parallel}
\end{numberedframe}

\protoslide{MPI_Comm_size}
\protoslide{MPI_Comm_rank}

\begin{mpl}
  \mplprotoslide{size}
  \mplprotoslide{rank}
\end{mpl}

\begin{numberedframe}{About routine signatures: C/C++}
  \label{sec:protos}
Signature:
\lstset{language=C}
\begin{lstlisting}
int MPI_Comm_size(MPI_Comm comm,int *nprocs)
\end{lstlisting}
Use:
\lstset{language=C}
\begin{lstlisting}
MPI_Comm comm = MPI_COMM_WORLD;
int nprocs;
int errorcode;
errorcode = MPI_Comm_size( comm,&nprocs );
\end{lstlisting}
(but forget about that error code most of the time)
\end{numberedframe}

\begin{fortran}
  \addtocounter{slidecount}{-1}

\begin{numberedframe}{About routine signatures: Fortran2008}
Signature
\lstset{language=Fortran}
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Type(MPI_Comm), INTENT(IN) :: comm
INTEGER, INTENT(OUT) :: size
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
\end{lstlisting}
Use:
\begin{lstlisting}
Type(MPI_Comm) :: comm = MPI_COMM_WORLD
integer :: size
CALL MPI_Comm_size( comm, size ) ! F2008 style
\end{lstlisting}
\begin{itemize}
\item final parameter optional.
\item \n{MPI_...} types are \n{Type}.
\end{itemize}
\end{numberedframe}

\begin{numberedframe}[b]{About routine signatures: Fortran90}
Signature
\lstset{language=Fortran}
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Integer, Intent(in) :: comm
Integer, Intent(out) :: ierror
\end{lstlisting}
Use:
\begin{lstlisting}
Integer :: comm = MPI_COMM_WORLD
Integer :: size,ierr
CALL MPI_Comm_size( comm, size, ierr ) ! F90 style
\end{lstlisting}
\begin{itemize}
\item Final parameter always error parameter. Do not forget!
\item \n{MPI_...} types are \n{INTEGER}.
\end{itemize}
\end{numberedframe}
\end{fortran}

\begin{python}
  \addtocounter{slidecount}{-1}
\begin{numberedframe}{About routine signatures: Python}
Signature:
\lstset{language=Python}
\begin{lstlisting}
# object method
MPI.Comm.Send(self, buf, int dest, int tag=0)
# class method
MPI.Request.Waitall(type cls, requests, statuses=None)
\end{lstlisting}
Use:
\begin{lstlisting}
from mpi4py import MPI
comm = MPI.COMM_WORLD
comm.Send(sendbuf,dest=other)
MPI.Request.Waitall(requests)
\end{lstlisting}
Note: most functions are methods of the \lstinline{MPI.Comm} class.\\
(Sometimes of \lstinline{MPI}, sometimes other.)
\end{numberedframe}
\end{python}

\begin{numberedframe}{Have you been paying attention?}
  \input{ex:mpi01rank}
\end{numberedframe}

\begin{exerciseframe}[commrank]
  \input ex:hello3
\end{exerciseframe}

\begin{exerciseframe}[commrank]
  \input ex:hello4
\end{exerciseframe}

\begin{numberedframe}{Processor name}
  Processes (can) run on physically distinct locations.

  \cverbatimsnippet{procname}
\end{numberedframe}

\begin{numberedframe}{In a picture}
  Four processes on two nodes (\n{idev -N 2 -n 4})
  \includegraphics[scale=.3]{node-comm-rank}
\end{numberedframe}

\protoslide{MPI_Get_processor_name}
\begin{mpl}
  \mplprotoslide{MPI_Get_processor_name}
\end{mpl}

\begin{exerciseframe}
  \input ex:procname
\end{exerciseframe}

\sectionframe{A practical example}

\begin{numberedframe}{Functional Parallelism}
  Parallelism by letting each process do a different thing.

  Example: divide up a search space.

  Each process knows its rank, so it can find its part of the search space.
\end{numberedframe}

\begin{exerciseframe}[prime]
  \input ex:primetest
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:array-ints}
\end{exerciseframe}

\endinput

\begin{numberedframe}\frametitle{}
\begin{lstlisting}
  
\end{lstlisting}
\end{numberedframe}

