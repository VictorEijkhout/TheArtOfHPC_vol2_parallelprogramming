% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-2022
%%%%
%%%% SPMD-slides.tex : slides about MPI's SPMD mode
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Overview}
  In this section you will learn how to think about parallelism in
  MPI.

  Commands learned:
  \begin{itemize}
  \item
    \indexmpishow{MPI_Init}, \indexmpishow{MPI_Finalize},
  \item 
    \indexmpishow{MPI_Comm_size}, \indexmpishow{MPI_Comm_rank}
  \item 
    \indexmpishow{MPI_Get_processor_name}, 
  \end{itemize}
\end{numberedframe}

\sectionframe{The MPI worldview: SPMD}

\begin{numberedframe}{Computers when MPI was designed}
  \includegraphics[scale=.1]{mpi-node1}

  One processor and one  process per node;\\
  all communication goes through the network.
\end{numberedframe}

\begin{numberedframe}{Pure MPI}
  \includegraphics[scale=.1]{mpi-node2}

  A node has multiple sockets, each with multiple cores.\\
  Pure MPI puts a process on each core: pretend shared memory doesn't exist.
\end{numberedframe}

\begin{numberedframe}{Quad socket node}
  \includegraphics[scale=.8]{ranger-node-small}
\end{numberedframe}

\begin{numberedframe}{Hybrid programming}
  \includegraphics[scale=.1]{mpi-node3}

  Hybrid programming puts a process per node or per socket;\\
  further parallelism comes from threading.\\
  Not in this course\ldots
\end{numberedframe}

\begin{numberedframe}{Terminology}
  `Processor' is ambiguous: is that a chip or one independent
  instruction processing unit?
  \begin{itemize}
  \item Socket: the processor chip
  \item Processor: we don't use that word
  \item Core: one instruction-stream processing unit
  \item Process: preferred terminology in talking about MPI.
  \end{itemize}  
\end{numberedframe}

\begin{numberedframe}{SPMD}
  The basic model of MPI is\\
  `Single Program Multiple Data':\\
  each process is an instance of the same program.

  Symmetry: There is no `master process', all processes are equal,
  start and end
  at the same time.

  Communication calls do not see the cluster structure:\\
  data sending/receiving is the same for all neighbors.
\end{numberedframe}

\sectionframe{Practicalities}

\begin{numberedframe}{Compiling and running}
  MPI compilers are usually called \indextermtt{mpicc},
  \indextermtt{mpif90}, \indextermtt{mpicxx}.

  These are not separate compilers,
  but scripts around the regular C/Fortran compiler. You can use all
  the usual flags.

  Running your program:
\begin{tacc}
  At TACC:\\
  \verb+ibrun yourprog+\\
  the number of processes is determined by SLURM.
\end{tacc}
general case:
\begin{verbatim}
mpiexec -n 4 hostfile ... yourprogram arguments
mpirun -np 4 hostfile ... yourprogram arguments
\end{verbatim}
\end{numberedframe}

\begin{numberedframe}{Do I need a supercomputer?}
  \begin{itemize}
  \item With \n{mpiexec} and such, you start a bunch of processes that
    execute your MPI program.
  \item Does that mean that you need a cluster or a big multicore?
  \item No! You can start a large number of MPI processes, even on
    your laptop. The OS will use `time slicing'.
  \item Of course it will not be very efficient\ldots
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Cluster setup}
  \small
  Typical cluster:
  \begin{itemize}
  \item Login nodes, where you ssh into; usually shared with 100 (or
    so) other people. You don't run your parallel program there!
  \item Compute nodes: where your job is run. They are often exclusive
    to you: no other users getting in the way of your program.
  \end{itemize}
  Hostfile: the description of where your job runs. Usually generated
  by a \indexterm{job scheduler}.
\end{numberedframe}

\begin{numberedframe}{Interactive run}
  \begin{itemize}
  \item Do not run your programs on a login node.
  \item Acquire compute nodes with \indextermtt{idev} or \indextermtt{qsub}\n{ -I}.
  \item Caveat: only small short jobs; nodes may not be available.
  \end{itemize}
  \includegraphics[scale=.15]{job_idev}
\end{numberedframe}

\begin{numberedframe}{Batch run}
  \begin{itemize}
  \item Submit batch job with \indextermtt{sbatch} or \indextermtt{qsub}
  \item Your job will be executed~\ldots Real Soon Now.
  \item See userguide for details about queues, sizes, runtimes,~\ldots
  \end{itemize}
  \includegraphics[scale=.15]{job_sbatch}
\end{numberedframe}

\begin{tacc}
  \begin{numberedframe}{Lab setup}
    \begin{itemize}
    \item Open a terminal window on a TACC cluster.
    \item Type \n{idev -N 2 -n 32 -t 4:0:0 } which gives
      you an interactive session of 2~nodes, 32~cores, for the next
      4~hours.
    \item (After this course, for serious work, you would write a
      batch script. The idev sessions are strictly limited in time and
      resources.)
    \item See the handout for reservations, project IDs, and location of training materials.
    \item Next slide for how to make and run exercises.
    \end{itemize}
  \end{numberedframe}
\end{tacc}

\begin{numberedframe}{How to make exercises}
  \begin{itemize}
  \item Directory: \n{exercises-mpi-c} or \n{cxx} or \n{f} or \n{f08}
    or \n{p} or \n{mpl}
  \item If a slide has a \n{(exercisename)} over it, there will be a
    template program \n{exercisename.c} (or \n{F90} or \n{py}).
  \item Type \n{make exercisename} to compile it
  \item Python: setup once per session
\begin{verbatim}
module load python3
\end{verbatim}
No compilation needed. Run:
\begin{verbatim}
ibrun python3 yourprogram.py
\end{verbatim}
\item Add an exercise of your own to the makefile: add the name to
    the \n{EXERCISES}
  \end{itemize}
\end{numberedframe}

\begin{exerciseframe}[hello]
  \input ex:hello1
\end{exerciseframe}

\begin{numberedframe}{In a picture}
  \includegraphics[scale=.35]{hello-parallel}
\end{numberedframe}

\sectionframe{We start learning MPI!}

\begin{numberedframe}{MPI headers: C}
\label{sl:mpi-header-c}
You need an include file:
\begin{verbatim}
#include "mpi.h"
\end{verbatim}
This defines all routines and constants.
\end{numberedframe}

\begin{fortran}
\begin{numberedframe}{MPI headers: Fortran}
\label{sl:mpi-header-f}
You need an include file:
\begin{verbatim}
use mpi_f08   ! for Fortran2008
use mpi       ! for legacy Fortran90
\end{verbatim}
\begin{itemize}
\item True Fortran bindings as of the 2008 standard.
\begin{tacc}
Provided in Intel compiler:
\begin{verbatim}
module load intel/18.0.2
\end{verbatim}
or newer. Not in gcc7.
\end{tacc}
(Legacy bindings are far inferior)
\end{itemize}
\end{numberedframe}
\end{fortran}

\begin{python}
\begin{numberedframe}{MPI headers: Python}
\label{sl:mpi-header-p}
You need an include file:
\begin{verbatim}
from mpi4py import MPI
\end{verbatim}
\end{numberedframe}
\end{python}

\begin{numberedframe}{MPI headers: C++}
\label{sl:mpi-header-cpp}
You need an include file:
\begin{itemize}
\item There are no real C++ bindings, but see MPL.
\end{itemize}
\begin{verbatim}
module load mpl

mpicc yourprogram.cpp -I${TACC_MPL_INC}

#include <mpl/mpl.hpp>
\end{verbatim}
\end{numberedframe}

\begin{numberedframe}{MPI Init / Finalize}
Then put these calls around your code:
\lstset{language=C}
\begin{lstlisting}
MPI_Init(&argc,&argv); // zeros allowed
// your code
MPI_Finalize();  
\end{lstlisting}
\end{numberedframe}

\begin{fortran}
\begin{numberedframe}{Init Finalize, Fortran}
and for Fortran:
\lstset{language=Fortran}
\begin{lstlisting}
call MPI_Init()     ! F08 style
! your code
call MPI_Finalize()

call MPI_Init(ierr) ! F90 style
! your code
call MPI_Finalize(ierr)
\end{lstlisting}  
\end{numberedframe}
\end{fortran}

\begin{numberedframe}{About errors}
  MPI routines return invoke an error handler (slide~\ref{sl:errhandler})\\
  return integer error code
  \begin{itemize}
  \item In C: function result. 
\lstset{language=C++}
\begin{lstlisting}
ierr = MPI_Init(0,0);
if (ierr!=MPI_SUCCESS) /* do something */
\end{lstlisting}
  But really: can often be ignored; is ignored in this course.
\begin{lstlisting}
MPI_Init(0,0);
\end{lstlisting}
  \item In Fortran: as optional (F08 only) parameter.
  \item In Python: throwing exception.
  \end{itemize}
  There's actually not a lot you can do with an error code:\\
  very hard to recover from errors in parallel.\\
  By default code bombs with (hoperfully informative) message.
\end{numberedframe}

\begin{python}
\begin{numberedframe}{Python bindings}
\begin{verbatim}
module load python3
\end{verbatim}
\begin{verbatim}
from mpi4py import MPI
\end{verbatim}
Run:
\begin{tacc}
\begin{verbatim}
ibrun python3 yourprogram.py
\end{verbatim}
\end{tacc}
Your local installation:
\begin{verbatim}
mpiexec -n 15 python yourprogram.py
\end{verbatim}
No initialization needed.
\end{numberedframe}
\end{python}

\begin{numberedframe}{C++ bindings}
  MPI-1 had C++ bindings, by MPI-2 they were deprecated, in MPI-3 they have been removed.
  \begin{itemize}
  \item Easy solution: use the C~bindings unaltered.
    \begin{itemize}
    \item This is done in the \n{cxx} exercise directory.
    \item Ugly: very un-OO.
    \end{itemize}
  \item There are C++ bindings in Boost. No longer developed?
  \item Try MPL: \url{https://github.com/rabauke/mpl}
    \begin{itemize}
    \item Very modern OO.
    \item Exercises in \n{mpl} directory.
    \item Caution: not a full MPI implementation\\
      (I/O and one-sided mostly missing)
    \end{itemize}
  \end{itemize}
\end{numberedframe}

\begin{exerciseframe}[hello]
  \input ex:hello2
\end{exerciseframe}

\begin{numberedframe}{Process identification}
  \label{sl:comm-world}
Every process has a number (with respect to a communicator)
\lstset{language=C}
\begin{lstlisting}
int MPI_Comm_size( MPI_Comm comm, int *nprocs )
int MPI_Comm_rank( MPI_Comm comm, int *procno )
\end{lstlisting}
Lowest number is always zero.

This is a logical view of parallelism: mapping to physical
processors/cores is invisible here.

For now, the communicator will be \indexmpishow{MPI_COMM_WORLD}.

\lstset{language=C}
\begin{lstlisting}
MPI_Comm comm = MPI_COMM_WORLD;
\end{lstlisting}

\lstset{language=Fortran}
\begin{lstlisting}
Type(MPI_Comm) :: comm = MPI_COMM_WORLD
\end{lstlisting}

\lstset{language=Python}
\begin{lstlisting}
from mpi4py import MPI
comm = MPI.COMM_WORLD
\end{lstlisting}

\lstset{language=C}
\end{numberedframe}

\begin{numberedframe}{Illustration}
  \includegraphics[scale=.35]{rank-parallel}
\end{numberedframe}

\begin{numberedframe}{About routine signatures: C/C++}
  \label{sec:protos}
Signature:
\lstset{language=C}
\begin{lstlisting}
int MPI_Comm_size(MPI_Comm comm,int *nprocs)
\end{lstlisting}
Use:
\lstset{language=C}
\begin{lstlisting}
MPI_Comm comm = MPI_COMM_WORLD;
int nprocs;
int errorcode;
errorcode = MPI_Comm_size( comm,&nprocs );
\end{lstlisting}
(but forget about that error code most of the time)
\end{numberedframe}

\begin{fortran}
\begin{numberedframe}{About routine signatures: Fortran2008}
Signature
\lstset{language=Fortran}
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Type(MPI_Comm), INTENT(IN) :: comm
INTEGER, INTENT(OUT) :: size
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
\end{lstlisting}
Use:
\begin{lstlisting}
Type(MPI_Comm) :: comm = MPI_COMM_WORLD
integer :: size
CALL MPI_Comm_size( comm, size ) ! F2008 style
\end{lstlisting}
\begin{itemize}
\item final parameter optional.
\item \n{MPI_...} types are \n{Type}.
\end{itemize}
\end{numberedframe}
\end{fortran}

\begin{fortran}
\begin{numberedframe}{About routine signatures: Fortran90}
Signature
\lstset{language=Fortran}
\begin{lstlisting}
MPI_Comm_size(comm, size, ierror)
Integer, Intent(in) :: comm
Integer, Intent(out) :: ierror
\end{lstlisting}
Use:
\begin{lstlisting}
Integer :: comm = MPI_COMM_WORLD
Integer :: size,ierr
CALL MPI_Comm_size( comm, size, ierr ) ! F90 style
\end{lstlisting}
\begin{itemize}
\item Final parameter always error parameter. Do not forget!
\item \n{MPI_...} types are \n{INTEGER}.
\end{itemize}
\end{numberedframe}
\end{fortran}

\begin{python}
\begin{numberedframe}{About routine signatures: Python}
Signature:
\lstset{language=Python}
\begin{lstlisting}
# object method
MPI.Comm.Send(self, buf, int dest, int tag=0)
# class method
MPI.Request.Waitall(type cls, requests, statuses=None)
\end{lstlisting}
Use:
\begin{lstlisting}
from mpi4py import MPI
comm = MPI.COMM_WORLD
comm.Send(sendbuf,dest=other)
MPI.Request.Waitall(requests)
\end{lstlisting}
Note: most functions are methods of the \lstinline{MPI.Comm} class.\\
(Sometimes of \lstinline{MPI}, sometimes other.)
\end{numberedframe}
\end{python}

\begin{numberedframe}{In a picture}
  Four processes on two nodes (\n{idev -N 2 -n 4})
  \includegraphics[scale=.3]{node-comm-rank}
\end{numberedframe}

\begin{numberedframe}{Processor identification}
  \begin{itemize}
  \item Processors are organized in `communicators'.
  \item For now only the `world' communicator
    (slide~\ref{sl:comm-world})
  \item Each process has a `rank' wrt the communicator.
  \end{itemize}
\end{numberedframe}

\protoslide{MPI_Comm_size}
\protoslide{MPI_Comm_rank}

\begin{numberedframe}{Have you been paying attention?}
  \input{ex:mpi01rank}
\end{numberedframe}

\begin{exerciseframe}[commrank]
  \input ex:hello3
\end{exerciseframe}

\begin{exerciseframe}[commrank]
  \input ex:hello4
\end{exerciseframe}

\begin{numberedframe}{Processor name}
  Processes (can) run on physically distinct locations.

  \cverbatimsnippet{procname}
\end{numberedframe}

\protoslide{MPI_Get_processor_name}

\begin{exerciseframe}
  \input ex:procname
\end{exerciseframe}

\sectionframe{A practical example}

\begin{numberedframe}{Functional Parallelism}
  Parallelism by letting each process do a different thing.

  Example: divide up a search space.

  Each process knows its rank, so it can find its part of the search space.
\end{numberedframe}

\begin{exerciseframe}[prime]
  \input ex:primetest
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:array-ints}
\end{exerciseframe}

\endinput

\begin{numberedframe}\frametitle{}
\begin{lstlisting}
  
\end{lstlisting}
\end{numberedframe}

