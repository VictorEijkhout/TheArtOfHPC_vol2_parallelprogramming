% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-2022
%%%%
%%%% Collective-slides.tex : slides about collective operations
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Overview}
  In this section you will learn `collective' operations, that combine
  information from all processes.

  Commands learned:
  \begin{itemize}
  \item \indexmpishow{MPI_Bcast}, \indexmpishow{MPI_Reduce}, \indexmpishow{MPI_Gather}, \indexmpishow{MPI_Scatter}
  \item \indexmpishow{MPI_All_...} variants, \indexmpishow{MPI_....v} variants
  \item \indexmpishow{MPI_Barrier}, \indexmpishow{MPI_Alltoall}, \indexmpishow{MPI_Scan}
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Technically}
  Routines can be `collective on a communicator': 
  \begin{itemize}
  \item They involve a communicator;
  \item if one process calls that routine, every process in that
    communicator needs to call it
  \item Mostly about combining data, but also opening shared files,
    declaring `windows' for one-sided communication.
  \end{itemize}
\end{numberedframe}

\sectionframe{Concepts}

\begin{numberedframe}{Collectives}
  Gathering and spreading information:
  \begin{itemize}
  \item Every process has data, you want to bring it together;
  \item One process has data, you want to spread it around.
  \end{itemize}
  Root process: the one doing the collecting or disseminating.

  Basic cases:
  \begin{itemize}
  \item Collect data: gather.
  \item Collect data and compute some overall value (sum, max): reduction.
  \item Send the same data to everyone: broadcast.
  \item Send individual data to each process: scatter.
  \end{itemize}
\end{numberedframe}

\frame{\includegraphics[scale=.6]{collective_comm}}

\begin{exerciseframe}
  \input ex:collective-cases
\end{exerciseframe}

\sectionframe{Basic collectives}

\begin{numberedframe}{Allreduce: reduce-to-all}
  \begin{itemize}
  \item \lstinline{MPI_Allreduce} does the same as:\\
    \lstinline{MPI_Reduce} (reduction) followed by \lstinline{MPI_Bcast} (broadcast)
  \item One line less code
  \item Same running time as either, half of reduce-followed-by-broadcast
  \item Expresses the symmetrical nature of the algorithm\\
    (And you don't have to think about who is the root)
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Motivation for allreduce}
  Example: normalizing a vector
  \[ y \leftarrow x/\|x\| \]
  \begin{itemize}
  \item Vectors \lstinline{x,y} are distributed: every process has certain
    elements
  \item The norm calculation is an all-reduce: every process gets same
    value
  \item Every process scales its part of the vector.
  \item Question: what kind of reduction do you use for an inf-norm? One-norm? Two-norm?
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Another Allreduce}
Standard deviation:
\[ \sigma = \sqrt{\frac1N \sum_i^N (x_i-\mu) }
\qquad\hbox{where}\qquad \mu = \frac{\sum_i^Nx_i}N
\]
and assume that every processor stores just one~$x_i$ value.

How do we compute this?
\begin{enumerate}
\item The calculation of the average~$\mu$ is a reduction.
\item Every
  process needs to compute~$x_i-\mu$ for its value~$x_i$, so use
  allreduce operation, which does the reduction and leaves
  the result on all processors.
\item $\sum_i(x_i-\mu)$ is another sum of
  distributed data, so we need another reduction operation. Might as
  well use allreduce.
\end{enumerate}
\end{numberedframe}

\begin{numberedframe}{Allreduce syntax}
\begin{lstlisting}
  int MPI_Allreduce(
    const void* sendbuf,
    void* recvbuf, int count, MPI_Datatype datatype,
    MPI_Op op, MPI_Comm comm)  
\end{lstlisting}
\begin{itemize}
\item All processes have send and recv buffer
\item (No root argument)
\item \lstinline{count} is number of items in the buffer: 1~for scalar.\\
  $>1$: pointwise application of the operator
\item \indexmpishow{MPI_Datatype} is \indexmpishow{MPI_INT}, \indexmpishow{MPI_REAL8} et cetera.
\item \indexmpishow{MPI_Op} is \indexmpishow{MPI_SUM}, \indexmpishow{MPI_MAX} et cetera.
\end{itemize}
\end{numberedframe}

\protoslide{MPI_Allreduce}

\begin{numberedframe}{Elementary datatypes}
\begin{tabular}{|l|l|l|}
  \hline
  C&Fortran&meaning\\
  \hline
  \indexmpishow{MPI_CHAR}&  \indexmpishow{MPI_CHARACTER}&only for text\\
  \indexmpishow{MPI_SHORT}&  \indexmpishow{MPI_BYTE}&8 bits\\
  \indexmpishow{MPI_INT}&  \indexmpishow{MPI_INTEGER}&like the C/F types\\
  \indexmpishow{MPI_FLOAT}&  \indexmpishow{MPI_REAL}&\\
  \indexmpishow{MPI_DOUBLE}&  \indexmpishow{MPI_DOUBLE_PRECISION}&\\
  &\indexmpishow{MPI_COMPLEX}&\\
  &\indexmpishow{MPI_LOGICAL}&\\  
  \hline
  unsigned&extensions&\\
  \hline
  &&\indexmpishow{MPI_Aint}\\
  &&\indexmpishow{MPI_Offset}\\
  \hline
\end{tabular}  

A bunch more.
\end{numberedframe}

\begin{numberedframe}{MPI operators}
\begingroup \tt\catcode`\_=12\relax %pyskip
\begin{tabular}{|l|l|}
  \hline
  \indexmpishow{MPI_Op}&description\\
  \hline
  \indexmpishow{MPI_MAX}&maximum\\
  \indexmpishow{MPI_MIN}&minimum\\
  \indexmpishow{MPI_SUM}&sum\\
  \indexmpishow{MPI_PROD}&product\\
  \indexmpishow{MPI_LAND}&logical and\\
  \indexmpishow{MPI_BAND}&bitwise and\\
  \indexmpishow{MPI_LOR}&logical or\\
  \indexmpishow{MPI_BOR}&bitwise or\\
  \indexmpishow{MPI_LXOR}&logical xor\\
  \indexmpishow{MPI_BXOR}&bitwise xor\\
  \indexmpishow{MPI_MAXLOC}&location of max\\
  \indexmpishow{MPI_MINLOC}&location of min\\
  \hline
\end{tabular}
\endgroup %pyskip

A couple more.
\end{numberedframe}

\begin{c}
\begin{numberedframe}{Buffers in C}
  General principle: buffer argument is address in memory of the data.
  \begin{itemize}
  \item Buffer is void pointer: 
  \item write \lstinline{&x} or \lstinline{(void*)&x} for scalar
  \item write \lstinline{x} or \lstinline{(void*)x} for array
  \end{itemize}
\end{numberedframe}
\end{c}

\begin{fortran}
\begin{numberedframe}{Buffers in Fortran}
  General principle: buffer argument is address in memory of the data.
  \begin{itemize}
  \item Fortran always passes by reference:
  \item write \lstinline{x} for scalar
  \item write \lstinline{x} for array
  \end{itemize}
\end{numberedframe}
\end{fortran}

\begin{c}
\begin{numberedframe}{Buffers in C++}
 \lstset{language=C++}
  \begin{itemize}
  \item Scalars same as in C.
  \item Use of \lstinline+std::vector+ or \lstinline+std::array+:
\begin{lstlisting}
vector<float> xx(25);
MPI_Send( xx.data(),25,MPI_FLOAT, .... );
MPI_Send( &xx[0],25,MPI_FLOAT, .... );
MPI_Send( &xx.front(),25,MPI_FLOAT, .... );
\end{lstlisting}
\item Can not send from iterator~/
  let recv determine size/capacity.
  \end{itemize}
 \lstset{language=C}
\end{numberedframe}
\end{c}

\begin{numberedframe}{Large buffers}
  As of \mpistandard{4} a buffer can be longer than $2^{31}$ elements.\\
  \begin{itemize}
  \item Use \indexmpishow{MPI_Count} for count
  \item In C: use \indexmpishow{MPI_Reduce_c}
  \item in Fortran: polymorphism means no change to the call.
  \end{itemize}
  \cverbatimsnippet{reducecount}
\end{numberedframe}

\begin{python}
\begin{numberedframe}{Buffers in Python}
  For many routines there are two variants:
  \begin{itemize}
  \item lowercase: can send Python objects;\\
    output is \lstinline{return} result\\
\begin{verbatim}
result = comm.recv(...)
\end{verbatim}
    this uses \n{pickle}: slow.
  \item uppercase: communicates \n{numpy} objects;\\
    input and output are function argument.
\begin{verbatim}
result = np.empty(.....)
comm.Recv(result, ...)
\end{verbatim}
    basicaly wrapper around C code: fast
  \end{itemize}
\end{numberedframe}
\end{python}

\begin{exerciseframe}[randommax]
  \input ex:randommaxscale
\end{exerciseframe}

\begin{comment}
\begin{numberedframe}{Random numbers in C}
\lstset{language=C}
\begin{lstlisting}
// Initialize the random number generator
srand(procno*(double)RAND_MAX/nprocs);
// compute a random number
randomfraction = (rand() / (double)RAND_MAX);
\end{lstlisting}
\end{numberedframe}

\begin{fortran}
\begin{numberedframe}{Random numbers in Fortran}
\lstset{language=Fortran}
\begin{lstlisting}
integer :: randsize
real :: random_value
call random_number(random_value)
\end{lstlisting}
\begin{lstlisting}
integer,allocatable,dimension(:) :: randseed
call random_seed(size=randsize)
allocate(randseed(randsize))
randseed(:) = 1023*procno
call random_seed(put=randseed)  
\end{lstlisting}
\lstset{language=C}
\end{numberedframe}
\end{fortran}

\begin{python}
\begin{numberedframe}{Random numbers in Python}
\lstset{language=Python}
\begin{lstlisting}
import random

random.seed(procno)

random_value = random.random()
\end{lstlisting}
\lstset{language=C}
\end{numberedframe}
\end{python}
\end{comment}

\begin{numberedframe}{Inner product calculation}
  Given vectors $x,y$:
  \[ x^ty = \sum_{i=0}^{N-1} x_iy_i \]
  Start out with distributed vectors~$x,y$,\\
  assume same distribution.

  Proposed solution:\\
  \indexmpishow{MPI_Gather} or \indexmpishow{MPI_Allgather}
  and calculate locally.

  Comments?
\end{numberedframe}

\begin{numberedframe}{Inner product calculation another way}
  What are (at least two) problems with:
\begin{lstlisting}
double local_prod[localsize],global_inprod[localsize];
for (i=0; i<localsize; i++)
    local_prod[i] = x[i]*y[i];
MPI_Allreduce( &local_prod, &global_inprod, 
               localsize,MPI_DOUBLE,MPI_SUM,comm )
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{Inner product calculation: the right way}
  Compute local part, then collect local sums.

\lstset{language=C}
\begin{lstlisting}
local_inprod = 0;
for (i=0; i<localsize; i++)
  local_inprod += x[i]*y[i];
MPI_Allreduce( &local_inprod, &global_inprod, 
               1,MPI_DOUBLE,MPI_SUM,comm )
\end{lstlisting}
\end{numberedframe}

\begin{comment}
  \begin{optexerciseframe}
    \input ex:randomcoord
  \end{optexerciseframe}
\end{comment}

\begin{numberedframe}{Reduction to single process}
    Regular reduce: great for printing out summary information at the
  end of your job.
\end{numberedframe}

\begin{numberedframe}{Reduction to root}
\lstset{language=C}
\begin{lstlisting}
int MPI_Reduce
  (void *sendbuf, void *recvbuf,
   int count, MPI_Datatype datatype, 
   MPI_Op op, int root, MPI_Comm comm)
\end{lstlisting}
\begin{itemize}
\item Buffers: \lstinline{sendbuf}, \lstinline{recvbuf} are ordinary variables/arrays.
\item Every process has data in its \lstinline{sendbuf},\\
  Root combines it in \lstinline{recvbuf} (ignored on non-root processes).
\item \lstinline{count} is number of items in the buffer: 1~for scalar.
\item \indexmpishow{MPI_Op} is \indexmpishow{MPI_SUM}, \indexmpishow{MPI_MAX} et cetera.
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Broadcast}
\begin{lstlisting}
int MPI_Bcast(
    void *buffer, int count, MPI_Datatype datatype, 
    int root, MPI_Comm comm )
\end{lstlisting}
\begin{itemize}
\item All processes call with the same argument list
\item \lstinline{root} is the rank of the process doing the broadcast
\item Each process allocates buffer space;\\
  root explicitly fills in values,\\
  all others receive values through broadcast call.
\item Datatype is \indexmpishow{MPI_FLOAT}, \indexmpishow{MPI_INT} et cetera, different
  between C/Fortran.
\item \lstinline{comm} is usually \indexmpishow{MPI_COMM_WORLD}
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Gauss-Jordan elimination}
  \url{https://youtu.be/aQYuwatlWME}
\end{numberedframe}

\begin{exerciseframe}[jordan]
  \small
  \input ex:gaussjordancoll
\end{exerciseframe}

\begin{optexerciseframe}
  Bonus exercise: can you extend your program to have multiple columns
  per processor?
\end{optexerciseframe}

\sectionframe{Scan}

\begin{numberedframe}{Scan}
Scan or `parallel prefix': reduction with partial results

\begin{itemize}
\item Useful for indexing operations:
\item Each process has an array of $n_p$ elements;
\item My first element has global number $\sum_{q<p}n_q$.
\item Two variants: \indexmpishow{MPI_Scan} inclusive, and
  \indexmpishow{MPI_Exscan} exclusive.
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{In vs Exclusive}
  \def\strut{\vrule height 25pt width 0pt\relax}
\[
\begin{array}{rccccc}
  \mathrm{process:}\strut
      &0&1&2&\cdots&p-1\\
  \mathrm{data:}\strut
      &x_0&x_1&x_2&\cdots&x_{p-1}\\
  \mathrm{inclusive:}\strut
      &x_0&x_0\oplus x_1&x_0\oplus x_1\oplus x_2&\cdots&\mathop\oplus_{i=0}^{p-1} x_i\\
  \mathrm{exclusive:}\strut
      &\mathrm{unchanged}&x_0&x_0\oplus x_1&\cdots&\mathop\oplus_{i=0}^{p-2} x_i\\
\end{array}
\]
\end{numberedframe}

\protoslide{MPI_Scan}

\begin{numberedframe}{For the next exercise}
  \label{fig:scanints}
  \includegraphics[scale=.5]{scanints}
\end{numberedframe}

\begin{exerciseframe}[scangather]
  \input ex:scanprint
\end{exerciseframe}

\sectionframe{Gather/Scatter, Barrier, and others}

\protoslide{MPI_Gather}
\protoslide{MPI_Scatter}

\begin{numberedframe}{Gather/Scatter}
\begin{itemize}
\item Compare buffers to reduce
\item Scatter: the \n{sendcount} / Gather: the \n{recvcount}:\\
this is not, as you might expect, the total length of the
buffer; instead, it is the amount of data to/from each process.
\end{itemize}
\end{numberedframe}

\begin{numberedframe}{Gather pictured}
  \includegraphics[scale=.4]{gather}
\end{numberedframe}

\begin{numberedframe}{Popular application of gather}
  Matrix is constructed distributed, but needs to be brougt to one
  process:
  
  \includegraphics[scale=.4]{allgathermatrix}

  This is not efficient in time or space. Do this
  only when strictly necessary. Remember SPMD: try to keep everything
  symmetrically parallel. 
\end{numberedframe}

\protoslide{MPI_Allgather}

\begin{numberedframe}{Allgather pictured}
  \includegraphics[scale=.4]{allgather}
\end{numberedframe}

\begin{numberedframe}{V-type collectives}
  \begin{itemize}
  \item Gather/scatter but with individual sizes
  \item Requires displacement in the gather/scatter buffer
  \end{itemize}
\end{numberedframe}

\protoslide{MPI_Gatherv}

\begin{exerciseframe}[scangather]
  \input ex:scangather
\end{exerciseframe}

\begin{reviewframe}
  An \indexmpishow{MPI_Scatter} call puts the same data on each process
  
  \slackpollTF+A scatter call puts the same data on each process+
\end{reviewframe}

\begin{numberedframe}{All-to-all}
  \begin{itemize}
  \item Every process does a scatter;
  \item (equivalently: every process gather)
  \item each individual data, but amounts are identical
  \item Example: data transposition in FFT
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Data transposition}
  \includegraphics[scale=.4]{alltoall}

  Example: each process knows who to send to, \\
  all-to-all gives information who to receive from
\end{numberedframe}

\begin{numberedframe}{All-to-allv}
  \begin{itemize}
  \item Every process does a scatter or gather;
  \item each individual data and individual amounts.
  \item Example: radix sort by least-significant digit.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Radix sort}
  \scriptsize
  Sort 4 numbers on two processes:

  \begin{tabular}{|r|rr|rrr|}
    \hline
    & \multicolumn{2}{|c}{proc0}&\multicolumn{3}{|c|}{proc1}\\
    array     & 2&5&7&1&\\
    binary    & 010& 101& 111& 001&\\
    \hline
    \multicolumn{6}{|c|}{stage 1}\\
    \hline
    last digit&   0&   1&   1&   1&\\
    &\multicolumn{5}{|c|}{(this serves as bin number)}\\
    sorted    & 010&    & 101& 111& 001\\
    \multicolumn{6}{|c|}{stage 2}\\
    \hline
    next digit&  1 &    &  0 &  1 &  0\\
    &\multicolumn{5}{|c|}{(this serves as bin number)}\\
    sorted    & 101& 001& 010& 111&\\
    \multicolumn{6}{|c|}{stage 3}\\
    \hline
    next digit& 1  & 0  & 0  & 1&\\
    &\multicolumn{5}{|c|}{(this serves as bin number)}\\
    sorted    & 001& 010& 101& 111&\\
    decimal   & 1  & 2  & 5  & 7&\\
    \hline
  \end{tabular}
\end{numberedframe}

\begin{numberedframe}{Reduce-scatter}
  \begin{itemize}
  \item 
    Pointwise reduction (one element per process) followed by scatter
  \item 
    Somewhat related to all-to-all: data transpose but reduced
    information, rather than gathered.
  \item Applications in both sparse and dense matrix-vector product.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Example: sparse matrix setup}
  Example: each process knows who to send to, \\
  all-to-all gives information how many messages to expect\\
  reduce-scatter leaves only relevant information

  \includegraphics[scale=.4]{reducescatter}
\end{numberedframe}

\begin{numberedframe}{Barrier}
\begin{lstlisting}
int MPI_Barrier( MPI_Comm comm )
\end{lstlisting}
  \begin{itemize}
  \item Synchronize processes:
  \item each process waits at the barrier until all processes have reached the barrier
  \item \textbf{This routine is almost never needed}:\\
    collectives are already a barrier of sorts, two-sided
    communication is a local synchronization
  \item One conceivable use: timing
  \end{itemize}
\end{numberedframe}

\sectionframe{User-defined operators}

\begin{numberedframe}{MPI Operators}
  Define your own reduction operator
  \begin{itemize}
  \item Define operator between partial result and new operand
\lstset{language=C}
\begin{lstlisting}
typedef void MPI_User_function
    ( void *invec, void *inoutvec, int *len, 
      MPI_Datatype *datatype); 
\end{lstlisting}
\item Don't forget to free:
\lstset{language=C}
\begin{lstlisting}
int MPI_Op_free(MPI_Op *op)  
\end{lstlisting}
\item Make your own reduction scheme \indexmpishow{MPI_Reduce_local}
  \end{itemize}
\end{numberedframe}

\begin{fortran}
\begin{numberedframe}{User defined operators, Fortran}
\lstset{language=Fortran}
\begin{lstlisting}
FUNCTION user_function( invec(*), inoutvec(*), length, mpitype)
<fortrantype> :: invec(length), inoutvec(length) 
INTEGER :: length, mpitype
\end{lstlisting}
\end{numberedframe}
\end{fortran}

\protoslide{MPI_Op_create}

\begin{numberedframe}{Example}
  Smallest nonzero:
  
  \cverbatimsnippet{mpirwz}
\end{numberedframe}

\begin{reviewframe}
  The $\|\cdot\|_2$ norm (sum of squares) needs a custom operator.
  
  \slackpollTF+The sum of squares norm needs a custom operators+
\end{reviewframe}

\sectionframe{Performance of collectives}

\begin{numberedframe}{Naive realization of collectives}
  Broadcast:
  
  \includegraphics[scale=.06]{graphics/bcast-simple}

  Single message:
  \[ \alpha=\hbox{message startup}\approx 10^{-6}s,\qquad
  \beta=\hbox{time per word}\approx 10^{-9}s
  \]
  \begin{itemize}
  \item Time for message of $n$ words: \[ \alpha +\beta n \]
  \item Time for collective? Can you improve on that?
  \end{itemize}

\end{numberedframe}

\begin{numberedframe}{Better implementation of collective}
  \includegraphics[scale=.07]{graphics/bcast-tree}
  
  \begin{itemize}
  \item
    What is the running time now?
  \item
    Can you come up with lower bounds on the $\alpha,\beta$ terms? Are
    these achieved here?
  \item How about the case of really long buffers?
  \end{itemize}

\end{numberedframe}

\begin{reviewframe}
  True of false: there are collectives that do not communicate data

  \slackpollTF+there are collectives that do not communicate data+
\end{reviewframe}

\subsectionframe{Reduction operators}

\begin{numberedframe}{User-defined operators}
\lstset{language=C}
Given a reduction function:
\begin{lstlisting}
typedef void user_function
    ( void *invec, void *inoutvec, int *len, 
      MPI_Datatype *datatype); 
\end{lstlisting}  
create a new operator:
\begin{lstlisting}
MPI_Op rwz;
MPI_Op_create(user_function,1,&rwz);
MPI_Allreduce(data+procno,&positive_minimum,1,MPI_INT,rwz,comm);
\end{lstlisting}
\end{numberedframe}

\begin{exerciseframe}[onenorm]
  \input ex:one-norm-op
\end{exerciseframe}

\endinput

\begin{numberedframe}{}
\begin{lstlisting}
  
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{}
\begin{lstlisting}
  
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{}
\begin{verbatim}
  
\end{verbatim}
\end{numberedframe}

