% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-2024
%%%%
%%%% ParLoop-slides.tex : slides about OpenMP's fork-join model
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Loop parallelism}
  Much of parallelism in scientific computing is in loops:
  \begin{itemize}
  \item Vector updates and inner products
  \item Matrix-vector and matrix-matrix operations
  \item Finite Element meshes
  \item Multigrid
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Work distribution}
  \begin{itemize}
  \item Suppose loop iterations are independent:
  \item Distribute them over the threads:
  \item Use \indextermtt{omp_get_thread_num} to determine disjoint subsets.
  \end{itemize}
\begin{lstlisting}
#pragma omp parallel
{
  int threadnum = omp_get_thread_num(),
    numthreads = omp_get_num_threads();
  int low = N*threadnum/numthreads,
    high = N*(threadnum+1)/numthreads;
  for (int i=low; i<high; i++)
    // do something with i
}
\end{lstlisting}
Discuss.
\end{numberedframe}

\begin{numberedframe}{Loops are easy}
C: directive followed by statement or block:
\begin{lstlisting}
#pragma omp parallel
{
#pragma omp for
  for ( int i=0; i<N; i++)
    ... something with i ...
}
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{Loops in Fortran}
Fortran: optional matching end directive
\begin{lstlisting}
!$omp parallel
!$omp do
  do i=1,n
    ... something with i ...
  end do
!$omp end parallel
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{Workshare constructs}
  Here's the two-step parallelization in OpenMP:
  \begin{itemize}
  \item You use the \indextermtt{parallel} directive to create a team of
    threads;
  \item then you use a `workshare' construct to distribute the
    work over the team.
  \item For loops that is the \indextermtt{for} (or \indextermtt{do}) construct.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Workshare construct for loops}
Parallel and workshare together
\begin{lstlisting}
#pragma omp parallel for
  for (i=0; i<N; i++)
    ... something with i ...
\end{lstlisting}
\end{numberedframe}

\begin{numberedframe}{Stuff inside a parallel region}
\begin{multicols}{2}  
\begin{lstlisting}
#pragma omp parallel
{
  code1();
  #pragma omp for
  for (int i=1; i<=4*N; i++) {
    code2();
  }
  code3();
}
\end{lstlisting}
\columnbreak
\includegraphics[scale=.07]{parallel-do}
\end{multicols}
\end{numberedframe}

\begin{numberedframe}{Loops are static}
  \begin{itemize}
  \item No \lstinline{break} or so.
  \item \lstinline{continue} or \lstinline{cycle} allowed.
  \item Fixed upper bound;
  \item Simple loop increment;
  \item $\Rightarrow$~OpenMP needs to be able to calculate
    the number of iterations in advance.
  \item No \lstinline{while} loops.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{When is a loop parallel?}
  It's up to you!

  This is parallelizable:
\begin{lstlisting}
  for (int i=low; i<hi; i++)
     x[i] = // expression without x
\end{lstlisting}
Is this?
\begin{lstlisting}
for (int i=low; i<hi; i++)
   x[ f(i) ] = // expression
\end{lstlisting}
Histograms / binning.
\end{numberedframe}

\begin{exerciseframe}
  \input{sl:omp:loop-over-2}
\end{exerciseframe}

\begin{exerciseframe}[vectorsum]
  Run the following snippet
  \cverbatimsnippet{exvectorsum}
  \begin{enumerate}
  \item Sequentially
  \item On one thread
  \item With more than one thread.
  \end{enumerate}
  Do a scaling study.
  What all can you do to improve performance? Read back through previous slides.
\end{exerciseframe}

\Level 1 {More about loops}

\begin{numberedframe}{Loop schedules}
  \begin{itemize}
  \item Static scheduling of iterations. \\
    (default in practice though not formally)\\
    Very efficient. Good if all iterations take the same amount of
    time.\\ \texttt{schedule(static)}
  \item Other possibility: dynamic.\\
    Runtime overhead; better if iterations do not take the same amount
    of time.\\
    \texttt{schedule(dynamic)}
  \end{itemize}

  Four threads, 8 tasks of decreasing size\\
  dynamic schedule is better:
  
  \includegraphics[scale=.07]{scheduling}
\end{numberedframe}

\begin{numberedframe}{Chunk size}
\small
  With $N$ iterations and $t$ threads:
  \begin{itemize}
  \item Static: each thread gets $N/t$ iterations.\\
    explicit chunk size: \texttt{schedule(static,123)}
  \item Dynamic: each thread gets $1$ iteration at a time\\
    explicit chunk size: \texttt{schedule(dynamic,45)}\\
  \end{itemize}
  \includegraphics[scale=.075]{schedules}
\end{numberedframe}

\begin{numberedframe}{Guided schedule}
  Use decreasing chunk size (with optional minimum
  chunk): \texttt{schedule(guided,6)}

  More flexible than dynamic, less overhead than dynamic.    
\end{numberedframe}

\begin{numberedframe}{Collapse}
  \begin{itemize}
  \item Parallelize loop nest
  \item More iterations $\Rightarrow$ better performance
  \item Inner loop needs to be directly nested\\
    bounds simple function of outer bounds and var
  \end{itemize}
\cverbatimsnippet{ompnbodycollapse}
\end{numberedframe}

\begin{numberedframe}{Ordered loops}
  \begin{itemize}
  \item Ordered iterations: normally OpenMP can execute iterations in
    any sequence.
    You can force ordering if you absolutely have to.
  \item Bad for performance and generally not needed.
  \item Use for I/O:
\begin{lstlisting}
#pragma omp parallel for ordered schedule(dynamic)
for ( i=... ) {
  x[i] = ...
  #pragma omp ordered
    printf(x[i])
}
\end{lstlisting}
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Barriers}
  \begin{itemize}
  \item Barrier: synchronize all threads
  \item explicit: \lstinline{omp barrier}
  \item implicit: end of a workshare construct such as \lstinline{for}
  \item remove implicit barriers: \lstinline{nowait} clause
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Nowait}
\begin{lstlisting}
#pragma omp parallel
{
  #pragma omp for nowait
  for ( int i=0; i<N; ++i )
    x[i] = // something
  #pragma omp for
  for ( int i=0; i<N; ++i )
    y[i] = ... x[i] ...
}
\end{lstlisting}
Needed:
  \begin{itemize}
  \item In the same parallel region (why?)
  \item Same number of iterations
  \item Same schedule
  \end{itemize}
\end{numberedframe}

\Level 1 {Loop data}

\begin{numberedframe}{Private vs shared}
Statements such as:
\begin{lstlisting}
#pragma omp parallel private(x)
#pragma omp parallel shared(y)
#pragma omp parallel default(none) shared(x) private(y)
\end{lstlisting}
  \begin{itemize}
  \item Shared data: comes from outside the parallel region
  \item Private data: either private versions of outside, or local defined
  \item Default is shared. Dangerous!
  \item Parallel loop variables are always private.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Where is the bug?}
\begin{lstlisting}
  int i,j;
  double temp;
#pragma omp parallel for private(temp)
   for(i=0;i<N;i++){
    for (j=0;j<M;j++){
       temp = b[i]*c[j];
       a[i][j] = temp * temp + d[i];
} }  
\end{lstlisting}
Is there a different way of handling \lstinline{temp}?
\end{numberedframe}

\begin{numberedframe}{More private/shared}
\begin{lstlisting}
float x=5.5;
#pragma omp parallel firstprivate(x)
  // x is now private, and has value 5.5

float y;
#pragma omp parallel for lastprivate(y)
for ( /* looping */ ) {
  // stuff
  y = something;
}
// now y has the sequentially last value
\end{lstlisting}
\end{numberedframe}

\Level 1 {Reduction}

\begin{exerciseframe}
  \input{ex:omp-procthreadn}
\end{exerciseframe}

\begin{numberedframe}{Shared memory problems}
Race condition: simultaneous update of shared data:
\begin{tabbing}
  process 1: \texttt{I=I+2}\\
  process 2: \texttt{I=I+3}
\end{tabbing}
Results can be indeterminate:

\tiny
\begin{tabular}{|rr|rr|rr|}
  \hline
  \multicolumn{2}{|c|}{scenario 1.}& \multicolumn{2}{|c|}{scenario 2.}&
  \multicolumn{2}{|c|}{scenario 3.}\\ \hline
  \multicolumn{6}{|c|}{$\n{I}=0$}\\ \hline
  read $\n{I}=0$&read $\n{I}=0$&
    read $\n{I}=0$&read $\n{I}=0$&
      read $\n{I}=0$& \\
  compute $\n{I}=2$&compute $\n{I}=3$& 
    compute $\n{I}=2$&compute $\n{I}=3$&
      compute $\n{I}=2$& \\
  write $\n{I}=2$& & &write $\n{I}=3$&write $\n{I}=2$& \\
  &write $\n{I}=3$&write $\n{I}=2$& & &read $\n{I}=2$\\
  &&&&&compute $\n{I}=5$\\
  &&&&&write $\n{I}=5$\\
  \hline
  \multicolumn{2}{|c|}{$\n{I}=3$}& \multicolumn{2}{|c|}{$\n{I}=2$}&
  \multicolumn{2}{|c|}{$\n{I}=5$}\\ \hline
\end{tabular}
\end{numberedframe}

\begin{numberedframe}{Reductions}
  \begin{itemize}
  \item Inner product loop:
    \cverbatimsnippet{inprodsum}
  \item Use the \indextermtt{reduction(+:s)} clause.
  \item All the usual operations are available; you can also make your own.
  \end{itemize}
\end{numberedframe}

\begin{exerciseframe}[pi]
  \footnotesize
  \input ex:omp-pi
\end{exerciseframe}

\begin{exerciseframe}[piadapt]
  \footnotesize
  \input ex:omp-pi-adapt
\end{exerciseframe}

\begin{numberedframe}{same exercise}
  \begin{enumerate}
  \item Use the \indextermtt{omp parallel for} construct to parallelize the loop.
    As in the previous lab, you may at first see an incorrect result.
    Use the \indextermtt{reduction} clause to fix this.
  \item Your code should now see a decent speedup, using up to 8~cores.
    However, it is possible to get completely linear speedup. For this
    you need to adjust the schedule.

    Start by using \indextermtt{schedule(static,$n$)}. Experiment with values
    for~$n$.  When can you get a better speedup? Explain this.
  \item Since this code is somewhat dynamic, try \indextermtt{schedule(dynamic)}.
    This will actually give a fairly bad result. Why?  Use
    \indextermtt{schedule(dynamic,$n$)} instead, and experiment with values
    for~$n$.
  \item Finally, use \indextermtt{schedule(guided)}, where OpenMP uses a
    heuristic.  What results does that give?
  \item \indextermtt{schedule(auto)} : leave it up to the system.
  \item \indextermtt{schedule(runtime)} : leave it up to environment variables;
    good for experimenting.
  \end{enumerate}
\end{numberedframe}

\begin{numberedframe}{Reduction on arrays, static}
  \cverbatimsnippet{creductstatic}
\end{numberedframe}

\begin{numberedframe}{Reduction on arrays, dynamic}
  \cverbatimsnippet{creductdynamic}
\end{numberedframe}

\begin{numberedframe}{Reduction on arrays, warning}
  Each thread gets a copy of the array on the stack\\
  $\Rightarrow$~possible stack overflow\\
  set \indexompshow{OMP_STACKSIZE}\\
  also see \indextermtt{ulimit} on the Unix level.
\end{numberedframe}

\Level 1 {Exercises}

\begin{exerciseframe}[jacobi]
  \input{ex:omp-jacobi1}
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:omp-jacobi2}
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:omp-jacobi3}
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:omp-jacobi4}
\end{exerciseframe}

\endinput

\begin{numberedframe}{}
\begin{lstlisting}
\end{lstlisting}
  \begin{itemize}
  \item 
  \end{itemize}
\end{numberedframe}

